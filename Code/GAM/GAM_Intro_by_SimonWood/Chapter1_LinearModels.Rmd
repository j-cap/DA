---
title: "Chapter 1 - Linear Models"
author: "Weber Jakob"
output: html_notebook
header-includes:
   - \usepackage{amsmath}
---
```{r}
library(gamair)
library(latex2exp)
```

The classical data fitting process starts with the famous linear model:
\begin{equation}
  \label{eq:1}
   y_i = \beta x_i + \epsilon_i, \quad i=1, ..., n \qquad \qquad (1.1)
\end{equation}
It is clear, that not all measured data is fit by this model. Given some variability, what can be 
inferred from these data? In particular:

1. What value of $\beta$ is the most consistent with the data?
2. What range of $\beta$ is consistent with the data?
3. Are some particular theoretically derived values of $\beta$ consistent with the data?

These questions are answered by the statistical linear model. 

## 1.1 A simple linear model
This section develops statistical methods for a simple linear model of the form (1.1). Formally, consider $n$ observations, $x_i, y_i$, where $y_i$ is an observation on random variable, $Y_i$, with expectation, $\mu_i = \mathbb{E}(Y_i)$. Suppose that an appropriate model for the relationship between $x$ and $y$ is
\begin{equation}
  Y_i = \mu_i + \epsilon_i, \quad \ where \ \mu_i = x_i \beta
\end{equation}
Here, $\beta$ is the unknown parameter and the $\epsilon_i$ are mutually independet zero mean random variables, each with the same variance $\sigma^2$. $Y$ is an example of a _response variable_, while $x$ is an example of a _predictor variable_. Such model is depicted in the following figure. 
![](images/cha1/lin_model.png)

### Simple least squares estimation
The basic approach to choose $\beta$ is using the _linear least squares method_, which minimizes the sum of squared error between the model output and the data. To do this, differentiate with respect to the variable (here $\beta$) and set it to zero. This leads to the _least squares estimate_ of $\beta$: 
\begin{equation}
  \hat \beta = \sum_{i=1}^n x_i y_i / \sum_{i=1}^n x_i^2
\end{equation}

### 1.1.1 Sampling properties of $\hat \beta$
To evaluate the reliability of $\beta$, we should consider some properties of the distribution of $\hat \beta$ values, which would be obtained from repeated independet replication of the $x_i, y_i$ data used for estimation. 
Here the concept of an _estimator_ is introduced, which is obtained by replacing the obersvations $y_i$ in the estimate of $\hat \beta$ by the random variables, $Y_i$. Therefore, the _estimator_, $\hat \beta$ is a random variable. The distribution of this random variable can now be discussed. Consider only the first two moments of that distribution. \
The expected value is $\mathbb{E}(\hat \beta) = \beta$. So, $\hat \beta$ is an unbiased estimator (== its expected value is equal to the true value of the parameter that it is supposed to estimate). \
The variability is $var(\hat \beta) = (\sum_{i=1}^n x_i^2)^{-1} \sigma^2$. In most circumstances, $\sigma^2$ itself is an unknown parameter and must also be estimated. 

### 1.1.2 How old is the universe?

```{r Age of the universe, echo=TRUE}
data(hubble)
# exclude datapoints 3 and 15 - they are outliers
hub.mod <- lm(hubble$y~hubble$x-1, data=hubble[-c(3,15)])
summary(hub.mod)
p <- ggplot(hubble, aes(x,y)) + geom_point() 
p <- p + geom_abline(aes(colour="abline", intercept = 0, slope = hub.mod$coefficients), col = "red")
p

```

```{r}
plot(fitted(hub.mod), residuals(hub.mod), xlab="fitted values", ylab = "residuals")
legend(x = "right", legend = c("Data", "Linear Model"))

```


The plot of fitted values ($\mu_i = \beta x_i$) vs. residuals ($\epsilon_i = y_i - \mu_i$) checks the assumption of random noise with zero mean. The plot should show an apparently random scatter of residuals around zero. 

### 1.1.3 Adding a distributional assumption
Further distributional assumptions are necessary to find confidence intervals for $\beta$, or test hypotheses related to the model. 
Specifically, assume that $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ for all i, which is equivalent to assuming $Y_i \sim \mathcal{N}(x_i\beta, \sigma^2)$. We have already seen that $\hat \beta$ is just a weighted sum of $Y_i$, which itself is a random variable. This makes $\hat \beta$ a random variable. Therefore, we have
$$
  \hat \beta \sim \mathcal{N}\big( \beta, (\sum x_i^2)^{-1} \sigma^2\big)  \qquad \qquad (1.4)
$$
**Testing hypotheses about $\beta$** \
Here, classic hypothesis testing is introduce:
We can test the null hypothesis, $H_0 : \beta = \beta_0$, versus the alternative hypothesis, $H_1 : \beta \neq \beta_0$, for some specified value $\beta_0$, by examining the probability of getting the observed $\hat \beta$ assuming $H_0$ to be true. 
If $\sigma^2$ were known we could work directly with (1.4), as follows. The probability required is known as *p-value* of the test. It is the probability of getting a value of $\hat \beta$ at least as favourable to $H_1$ as the one actually observed, if $H_0$ is actually true. The p-value is the
$$
  p = P\Big[ \frac{\vert \hat \beta - \beta_0 \vert}{\sigma_{\hat \beta}} \ge \frac{\vert \hat \beta_{obs} - \beta_0 \vert}{\sigma_{\hat \beta}}\Big \vert H_0 \Big] = P\Big[ \vert Z \vert \ge \vert z \vert \Big]
$$
where $\hat \beta_{obs}$ is the estimate, $\hat \beta$ is the estimator, $Z \sim \mathcal N(0,1)$, $z = (\hat \beta_{obs} - \beta_0)/\sigma_{\hat \beta}$ and $\sigma_{\hat \beta} = (\sum x_i^2)^{-1} \sigma^2$. Hence, having formed Z, the p-value is easily worked out using the cumulative distribution functionfor the standard normal. Small p-values suggest that the data are inconsistent with $H_0$, while large values indicate consistency. 
In reality $\sigma^2$ is usually unknown. Therefore, $\sigma$ is replaced by $\hat \sigma$. This introduces some extra uncertainty (unless the sample size is very large). It turns out that if $H_0 : \beta = \beta_0$ is true then
$$
  T \equiv \frac{\hat \beta - \beta_0}{\sigma_{\hat \beta}} \sim t_{n-1}
$$
where $n$ is the sample size and $t_{n-1}$ is the _t-distribution_ with $n-1$ degrees of freedom. Large magnitude values of T favour $H_1$, so using T as the test statistic, in place of $\hat \beta$ we can calulate the p-value by evaluating
$$
  p = P\Big[\vert T \vert > \vert t \vert \Big]
$$
where $T \sim t_{n-1}$ and $t = (\hat \beta_{obs} - \beta_0)/ \sigma_{\hat \beta}$. 
Here is some code to evaluate the p-value for $H_0 : \beta = 163000000$
```{r}
cs.hubble <- 163000000
t.stat <- (coef(hub.mod) - cs.hubble) / summary(hub.mod)$coefficients[2]
pt(t.stat, df=23)*2
```
As judged by the test statistic, $t$, the data would be hugely improbably if $\beta = 1.63*10^8$. 

Hypothesis testing is usefull when there are good reasons to want to stick with some null hypothesis, until there is good reason to reject it. 

**Confidence intervals** \
The natural question after testing whether a particular value of $\beta$ is consistent with the data, is _what range_ of values of $\beta$ would be consistent with the data. To do this, we need a definition of "consistent": a common choice is to say that any parameter value is consistent with the data if it results in a p-value of $\ge 0.05$, when used as the null value in a hypothesis test. \
The defining property of a 95% confidence interval is this: if we were to gather an infinite sequence of independent replicate data sets, and calulate 95% confidence intervals for $\beta$ from each, then 95% of these intervals would include the true $\beta$, and 5% would not. 
For the hubble example, a 95% CI for the hubble constant is given by:
```{r}
sigb <- summary(hub.mod)$coefficients[2]
h.ci <- coef(hub.mod) + qt(c(0.025, 0.975), df=21) * sigb
# now construct the confidence interval
h.ci <- h.ci*60^2*24*365.25/3.09e19 # convert to 1/years
sort(1/h.ci)
```
Actually this "Hubble age" is the age of the universe if it has been expanding freely, essentially unfettered by gravitation. If the universe is really ‘matter dominated’ then the galaxies should be slowed by gravity over time so that the universe would actually be younger than this, but it is time to get on with the subject of this book.


## 1.2 Linear models in general

The simple linear model can be generalized by allowing the response variable to depend on multiple variables (plus an additive constant). These extra predictor variables can themselves be transformations of the original predictors. Some examples are

1. $\mu_i = \beta_0 + x_i \beta_1, \quad Y_i = \mu_i + \epsilon_i$
2. $\mu_i = \beta_0 + x_i \beta_1 + x_i^2 \beta_2 + x_i^3 \beta_3, \quad Y_i = \mu_i + \epsilon$
3. $\mu_i = \beta_0 + x_i \beta_1 + z_i \beta_2 + log(x_i z_i) \beta_3, \quad Y_i = \mu_i + \epsilon_i$

Each of these is a linear model because the $\epsilon_i$ terms and the model parameters, $\beta_j$ enter the model in a linear way. The parameters can be estimated by the _least squares method_ by minimizing the sum of squared errors. The theory is entirely based on re-writing the linear model using matrices and vectors. The model has then the general form
$$
  \mathbf \mu = \mathbf X \mathbf \beta  
$$
,i.e. the expected value vector $\mu$ is given by a **model matrix** (also known as design matrix), **X**, multiplied by a parameter vector, $\beta$. All linear models can be written in this general form.

Models in which data are divided into different groups, each of which are assumed to have a different mean, can be written in the same way making use of so-called _factor variables_. 

## 1.3 The theory of linear models

This section shows how the parameters $\beta$ can be estimated by least squares. It is assumend that **X** is a matrix with $n$ rows (one per data point) and $p$ columns (one per predictor plus offset). It will be shown that the resulting estimator $\hat \beta$ is 

* unbiased
* has the lowest variance of any possible linear estimator of $\beta$
* $\hat \beta \sim \mathcal N(\beta, \big(\mathbf X^T \mathbf X)^{-1} \sigma^2 \big)$ given the normality of the data

Further, results are derived for setting confidence interavals and for testing hypotheses about parameters. 

### 1.3.1 Least squares estimation of $\beta$

Point estimates for the parameters $\beta$, can be optained by the method of least squares, that is by minimizing
$$
  \mathcal S = \sum_{i=1}^n (y_i - \mu_i)^2 = \lVert \mathbf y - \mathbf \mu \rVert^2 = \lVert \mathbf y - \mathbf X \mathbf \beta \rVert^2  
$$
with respect to $\beta$. The model matrix **X** can be decomposed like
$$
  \mathbf X = \mathbf Q \begin{bmatrix} \mathbf R \\
                                        \mathbf 0\\
                        \end{bmatrix} = \mathbf Q_f \mathbf R
$$
where **R** is a $p \times p$ upper triangular matrix and **Q** is an $n \times n$ orthogonal matrix, the first p columns of which form $\mathbf Q_f$. Orthogonal matrices rotate vectors, but do not change their length. After applying $\mathbf Q^T$  and some reformulation using $\mathbf Q^T \mathbf y = \begin{bmatrix} \mathbf f \\ \mathbf r \\ \end{bmatrix}$, the **least squares parameter estimate** can be written as
$$
    \hat \beta = \mathbf R^{-1} \mathbf f
$$

### 1.3.2 The distribution of $\beta$

The distribution of the estimator $\hat \beta$ follows from that of $\mathbf Q^T \mathbf y$. Multivariate normality of $\mathbf Q^T \mathbf y$ follows from that of **y**, and since the covariance matrix of **y** is $\mathbf I_n \sigma^2$,it follows that
$$  
  \mathbf V_{\mathbf Q^T \mathbf y} = \mathbf Q^T \mathbf I_n \mathbf Q \sigma^2 = \mathbf I_n \sigma^2
$$
Further it follows that 
$$
  \mathbf f \sim \mathcal(\mathbf R \mathbf \beta, \mathbf I_p \sigma^2) \quad and \quad \mathbf r \sim \mathcal N(0, \mathbf I_{n-p} \sigma^2)
$$
with both vectors independent of each other. Turning to the properties of $\hat \beta$ itself, unbiasedness follows from
$$
  \mathbb E(\hat \beta) = \mathbf R^{-1} \mathbb E(\mathbf f) = \mathbf R^{-1} \mathbf R \mathbf \beta = \mathbf \beta
$$
The covariance matrix of **f** is $\mathbf I_p \sigma^2$, therefore it follows that the covariance matrix of $\hat \beta$ is
$$
  \mathbf V_{\hat \beta} = \mathbf R^{-1} \mathbf I_p \mathbf R^{-T} \sigma^2 = \mathbf R^{-1} \mathbf R^{-T} \sigma^2. \qquad \qquad (1.7) 
$$
Furthermore, since $\hat \beta$ is just a linear transformation of the normal random varibles **f**, it must follow a multivariate normal distribution
$$
    \beta \sim \mathcal N(\beta, \mathbf V_{\hat \beta})
$$
This result is not usually directly useful for making inferences about $\beta$, since $\sigma^2$ is generally unknown and must be estimated, thereby introducing an extra component of variability that should be accounted for. 

### 1.3.3 $(\hat \beta_i - \beta_i) / \hat \sigma_{\beta_i} \sim t_{n-p}$

Statistical results for the distribution of $\mathbf r$, given that $\lVert \mathbf r \rVert = \lVert \mathbf y - \mathbf X \hat \beta \rVert$.  

### 1.3.4 F-ratio results

It is also of interest to obtain distributional results for testing, for example, the simultaneous equality to zero of several model parameters. Such tests are particularly useful for making inferences about factor variables and their interactions, since each factor (or interaction) is typically represented by several elements of $\beta$.

### 1.3.5 The influence matrix

This is the matrix $A$ which yields the fitted value vector $\hat \mu$, when post-multiplied by the data vector $y$. It is given by $\mathbf A = \mathbf Q_f \mathbf Q_f^T$, where $\mathbf Q_f$ is given by the first $p$ columns of $Q$. 
The trace of the influence matrix is the number of identifiable parameters in the model. Secondly, $AA = A$ which is called _idempotency_.

### 1.3.6 The residuals $\hat \epsilon$ and fitted value $\hat \mu$

The influence matrix helps deriving properties of the fitted values and residuals. $\hat \mu$ is unbiased, the distribution is degenerate multivariate normal. The residuals behaves similarly.

### 1.3.7 Results in terms of $\mathbf X$

So far, the results have been presented in form of the methods actually used to fit linear models. Historically, the results are more usually presented in terms of the model matrix $A$, rather than the components of the $QR$-Decomposition. The influence matrix can be written as $\mathbf A = \mathbf X (\mathbf X^T \mathbf X)^{-1} \mathbf X^T.

### 1.3.8 The Gauss Markov Theorem: what's so special about least squares?

The GMT shows that least squares estimators have the lowest variance of all unbiased estimators that are linear (the data only enter the estimation process in a linear way).


## 1.4 The geometry of linear modelling

A full understanding of what is happening when using a least squares fit is facilitated by  taking the a geometric view of the fitting process. 

## 1.5 Practical linear models

In general, practical linear modelling is concerned with finding an appropriate model to explain the relationship of a response (random) variable to some predictor variables.

