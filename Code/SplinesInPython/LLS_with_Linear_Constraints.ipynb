{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Practical Optimization](https://www.google.com/search?client=firefox-b-d&q=Gill%2C+P.E.%2C+Murray%2C+W.+and+Wright%2C+M.H.+%281981%29+Practical+Optimization)  \n",
    "\n",
    "by Philip Gill, Walter Murray and Margaret Wright\n",
    "\n",
    "# 5.2 Active set methods for linear inequality constraints\n",
    "\n",
    "\n",
    "## 5.3.2 Quadratic Programming\n",
    "The general form of a quadratic programming problem with only inequality constraints is\n",
    "$$\n",
    "\\mathcal{QP} \\quad \\min_{x \\in \\mathcal{R^n}} c^Tx + \\frac{1}{2}x^TGx \\ subject \\ to \\quad Ax \\ge b,\n",
    "$$\n",
    "for a constant matrix $G$ and vector $c$.\n",
    "\n",
    "Almost all major algorithms for quadratic programming are active set methods. This point is emphasized because even closely related algorithms may be described in widely different terms.\n",
    "For example, some QP methods are based on a 'tableau' involving the constraints and the matrix\n",
    "$G$, others involve 'pivots' , and so on. In describing $QP$ algorithms, we shall retain the notation introduced in Section 5.2.1. Thus, $\\hat A_k$ will denote the working set at the iterate $x_k$ , and $Z_k$ will denote a matrix whose columns form a basis for the set of vectors orthogonal to the rows of $\\hat A_k$.\n",
    "\n",
    "### 5.3.2.1 Positive-definite quadratic programming\n",
    "A positive-definite quadratic porgram is one in which the projected Hessian matrix $Z_k^T G Z_k$ is known *a priori* to be positive-definite at every iteration. In general, this will be if $G$ itself is positive-definite. This implies, that the search direction is always well-defined, and that the quadratic function has a unique minimum in the subspace defined by $Z_k$.\n",
    "\n",
    "The 'search direction' at $x_k$ is obtained by solving the Newton equations\n",
    "$$\n",
    "        Z_k^T G Z_k p_z = - Z_k^T g_k, \\quad (5.47)\n",
    "$$\n",
    "and setting $p_k = Z_k p_z$. There are only two choices for the step length because of the quadratic nature of the objective function: \n",
    "\n",
    "1. A step of unity along $p_k$ is the excat step to the minimum of the function restricted to the null space of $\\hat A_k$. If this step is possible, the enxt iterate will be a constrained stationary point with respect to the equality constraints defined by $\\hat A_k$, and exact Lagrange multipliers can be computed to determine wheter a constraint should be deleted.\n",
    "\n",
    "2. A step of less than unity along $p_k$ to the nearest constraint, and a new constraint will be included in the working set at the next iterate.\n",
    "\n",
    "\n",
    "\n",
    "## 5.3.3 Linear Least-Squares with Linear Constraints\n",
    "\n",
    "Closesly related to **quadratic programming**. \n",
    "Consider the constrained linear least-squares problem\n",
    "$$\n",
    "    \\min_{x \\in \\mathcal{R^n}} \\frac{1}{2} \\lVert H x - d \\rVert_2^2 \\\\\n",
    "    sub. \\ to \\quad Ax \\ge b,\n",
    "$$\n",
    "\n",
    "where $H \\in \\mathcal R^{s \\times n}$. Usually, $s \\gg n$. The problem LLS is simply a quadratic program of the from given in Section 5.3.2, with $G = H^TH$ and $c = H^T d$. However, methods can be developed to solve LLS that avoid explicit use of a matrix whose condition number is the square of that of $H$. \n",
    "\n",
    "Given a feasible point $x_k$ that exactly satisfies a set of linearly independent constraints (with associated matrix $\\hat A_k$), the minimum residual subject to the equalities defined by $\\hat A_k$ is achieved by taking the step $p_k$ that solves\n",
    "$$\n",
    "    Z_k^T H^T H p_z = -Z_k^T H^T d_k, \n",
    "$$\n",
    "where $d_k = H x_k - d$. These are just the normal equations for the unconstrained linear least-squares problem\n",
    "$$\n",
    "    \\min_{p_z \\in \\mathcal R^{n-t}} \\frac{1}{2} \\lVert H Z_k p_z - d_k \\rVert_2^2.\n",
    "$$\n",
    "It is assumed that $\\hat A_k$ contains enough constraints to ensure that the matrix $H Z_k$ is of full rank (this is equivalent to requiring that the least-squares subproblem has a unique solution).\n",
    "\n",
    "Let $P_k$ be an orthonormal matrix of dimension $s \\times s$. Then \n",
    "$$\n",
    "    \\lVert H Z_k p_z - d_k \\rVert_2^2 = \\lVert P_k(H Z_k p_z - d_k)\\rVert_2^2. \\quad (5.52)\n",
    "$$ \\\n",
    "Suppose that $P_k$ is chosen to be a matrix that transforms $H Z_k$ to upper-triangular form (see Section 2.2.5.3), i.e.\n",
    "$$\n",
    "    P_k H Z_k = \\left(\\begin{matrix}    R_k \\\\\n",
    "                                        0 \n",
    "                    \\end{matrix} \\right)\n",
    "$$\n",
    "where $R_k$ is a non-singular $(n - t_k) \\times (n - t_k)$ upper-triangular matrix. Then, from (5.52),\n",
    "$$\n",
    "    \\lVert P_k (H Z_k p_z - d_k) \\rVert_2^2 = \\lVert \\left(\\begin{matrix} R_k \\\\ 0                                                                        \\end{matrix}\\right)\n",
    "     p_z - P_k d_k \\rVert_2^2.\n",
    "$$\n",
    "From the last expression, we see that the residual vector of the transformed problem will be minimized when the components of $R_k p_z$ are equal to the first $n - t_k$ components of $P_k d_k$. The minimum residual is therefore achieved for the vector $p_z$ that is the unique solution of the linear system $R_k p_z = \\bar d_k$, where $\\bar d_k$ denotes the vector of first $n - t_k$ components fo $P_k d_k$ (see Section 2.2.5.3). \n",
    "\n",
    "It is possible to add or delete constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30769231, -0.69230769,  1.38461538])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qpsolvers example\n",
    "from qpsolvers import solve_qp\n",
    "import numpy as np\n",
    "\n",
    "M = np.array([[1., 2., 0.], [-8., 3., 2.], [0., 1., 1.]])\n",
    "P = np.dot(M.T, M) # symmetric matrix\n",
    "q = np.dot(np.array([3., 2., 3.]), M).reshape((3,))\n",
    "#G =    array([[1.,2.,1.],[2.,0.,1.], [-1.,2.,-1.]])\n",
    "G = np.array([[1.,2.,1.],[2.,0.,1.], [-1.,2.,-1.]])\n",
    "h = np.array([3., 2., -2.]).reshape((3,))\n",
    "A = np.array([1., 1.,1.])\n",
    "b = np.array([1.])\n",
    "\n",
    "s = solve_qp(P, q, G, h, A, b)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
