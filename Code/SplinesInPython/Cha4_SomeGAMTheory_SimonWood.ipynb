{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT to READ and IMPLEMENT\n",
    "\n",
    "- [ ] **Monotonicity and Convexity**: \n",
    "[Hofner 2012](http://benjaminhofner.de/downloads/2012/talks/BiometrischesKolloquium_Hofner.pdf)\n",
    "- [ ] **Automatic knot selection**: [Goepp 2018](https://arxiv.org/pdf/1808.01770.pdf) \n",
    "- [x] **Monotonicity** for *Cubic Regression Splines*: [Wood 1994](https://epubs-siam-org.uaccess.univie.ac.at/doi/pdf/10.1137/0915069)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Some GAM theory\n",
    "\n",
    "The problem of estimation a GAM becomes the problem of estimation *smoothing parameters* $\\lambda_i$ and *model coefficients* $\\beta_i$ for a *penalized likelihood maximization problem* once a *basis* for the smooth functions and a *measure of function wiggliness* has been chosen. In practice, this is solved by *P-IRLS*, while the smoothing parameters can be estimated using cross validation or related criteria. \n",
    "\n",
    "The methods discussed here are almost all built around penalized regression smoothers, based on splines. Some sources are\n",
    "\n",
    "- Wahba (1980) - groudwork on penalized regression smoothers\n",
    "- de Boor (1978) - introduced B-splines\n",
    "- Parker and Rice (1985) - groundwork on penalized regression smoothers\n",
    "- Hastie and Tibshirani (1990) - use P-Splines in the GAM context\n",
    "- Marx and Eilers (1998) - improved the use for B-Splines by introducing P-Splines with a B-Spline basis\n",
    "\n",
    "The main components of the framework for generalized additive modelling are covered in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Smoothing bases\n",
    "For simplicity of presentation, only one very simple type of penalized regression smoother was presented in Chapter 3. For practical work a variety of alternative smoothers are available, and this section introduces a useful subset of the possibilities, starting with smooths of one covariate, and then moving on to smooths of one or more covariates. Since all the smooths presented are based on splines (although the tensor product smooths need not be), the section starts by addressing the question: whatâ€™s so special about splines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Why splines?\n",
    "Some theoretical properties that make cubic splines appealing for penalized regression, first in the concext of interpolation, and then of smoothing.\n",
    "\n",
    "**Natural cubic splines are smoothest interpolators** \\\n",
    "Consider a set of points $\\{ x_i, y_i: i=1,...,n\\}$ where $x_i < x_{i+1}$. The *natural cubic spline*, $g(x)$, interpolating these points, is a function made up of sections of cubic polynomial, one for each $[x_i, x_{i+1}]$, which are joined together so that the whole spline is continous to second derivative, while $g(x_i) = y_i$ and $g''(x_1) = g''(x_n) = 0$.\n",
    "\n",
    "Of all functions that are continous on $[x_1, x_n]$, have continous first derivatives and interpolate $\\{x_i, y_i\\}$, the natural cubic spline $g(x)$ is the smoothest in the sense of minimizing\n",
    "$$\n",
    "    J(f) = \\int_{x_1}^{x_n} f''(x)^2 dx. \n",
    "$$\n",
    "The prove is given by Green and Silverman (1994), based on the original work of Schoenberg (1964). In de Boor (1978, Cha. 5) a number of results are presented showing that cubic spline interpolation is optimal, or at least very good.\n",
    "\n",
    "**Cubic smoothing splines** \\\n",
    "Usually $y_i$ is measured with noise, and it is generally more useful to smooth $\\{x_i, y_i\\}$ data, rather that interpolating them. Rather than setting $g(x_i) = y_i$, it might be better to treat the $g(x_i)$ as $n$ free parameters of a cubic spline, and to estimate them in order to minimize\n",
    "$$\n",
    "    \\sum_{i=1}^n \\big( y_i - g(x_i)\\big)^2 + \\lambda \\int g''(x)^2 dx, \n",
    "$$\n",
    "where $\\lambda$ is a tuneable parameter controling the relative weight of the conflicting goals of matching the data and producing a smooth $g$. The result $g(x)$ is a *smoothing spline* (Reinsch, 1967). Of all functions, that are continuous on $[x_1, x_n]$ and have cont. first derivatives, $g(x)$ is the function minimizing:\n",
    "$$\n",
    "    \\sum_{i=1}^n \\big( y_i - f(x_i)\\big)^2 + \\lambda \\int f''(x)^2 dx. \\quad (4.1)\n",
    "$$\n",
    "Smoothing splines seem to be ideal smoothers. The only problem is that they have as many free parameters as there are data to be smoothed. This is a problem as soon as we try to deal with more covariates/dimensions.\n",
    "\n",
    "An obvious compromise between retaining the good properties of splines, and computational efficiency, is to use penalized regression splines, as introduced in Chapter 3. At its simplest, this involves constructing a spline basis (and associated penalties) for a much smaller data-set than the one to be analyzed, and then using that basis\n",
    "(plus penalties) to model the original data set. The covariate values in the smaller data set should be arranged to nicely cover the distribution of covariate values in the original data set. This penalized regression spline idea is presented in Wahba (1980) and Parker and Rice (1985), for example. In the rest of this section, some spline based\n",
    "penalized regression smoothers will be presented, starting with univariate smoothers, and then moving on to smooths of several variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Cubic regression splines\n",
    "One approach is to parametrize the spline in terms of its values at the knots. Consider defining a cubic spline function, $f(x)$ with $k$ knots, $x_1,..., x_k$. Let $\\beta_j = f(x_j)$ and $\\delta_j = f''(x_j)$. Then the spline can be written as\n",
    "$$\n",
    "    f(x) = a_j^-(x)\\beta_j + a_j^+ \\beta_{j+1} + c_j^-(x) \\delta_j + c_j^+(x) \\delta_{j+1} \\quad if \\ x_j \\le x \\le x_{j+1} \\quad (4.2)\n",
    "$$\n",
    "for some special form of $a$ and $c$ given in the following figure. [Link to Coefficients](https://github.com/j-cap/Master-Thesis/blob/master/Images/wood-cubic-spline-coeff.PNG) \\\n",
    "With some work, the spline can be re-written entirely in terms of $\\beta$, which can then be re-written as \n",
    "$$\n",
    "    f(x) = \\sum_{i=1}^k b_i(x) \\beta_i.\n",
    "$$\n",
    "Hence, given a set of $x$ values at which to evaluate the spline, it is easy to obtain a model matrix mapping $\\beta$ to the evaluated spline. It is important to notice that in addition to having directly interpretable parameters, this basis does not require any re-scaling of the predictor variables before it can be used to construct a GAM. One only needs to choose the locations of the knots $x_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Cyclic cubic regression splines\n",
    "It is quite often appropriate for a model smooth function to be 'cyclic', meaning that the function has the same value and first few derviatives at its upper and lower boundaries. The penalized cubic regression spline can be modified to produce such a smooth. The spline can still be written in the form of (4.2), but we now have that $\\beta_1 = \\beta_k$ and $\\delta_1 = \\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.4 P-splines\n",
    "Yet another way to represent cubic splines (and indeed splines of higher or lower order), is by use of the **B-spline basis**. The B-spline basis is appealing because the basis functions are <code>strictly local</code> - each basis function is only non-zero over the intervals between $m + 3$ adjacent knots, where $m + 1$ is the order of the basis (e.g. $m = 2$ for a cubic spline). To define a $k$ parameter B-spline basis, we need to define $k+m+1$ knots, $x_1 < x_2 < . . . < x_{k+m+1}$, where the interval over which the spline is to be evaluated lies within $[x_{m+2}, x_k]$ (so that the first and last $m+1$ knot locations are essentially arbitrary). An $(m+1)^{th}$ order spline can be represented as\n",
    "$$\n",
    "    f(x) = \\sum_{i=1}^k B_i^m(x) \\beta_i,\n",
    "$$\n",
    "where the B-spline basis functions are most conveniently defined recursively as follows:\n",
    "\n",
    "$$\n",
    "    B_i^m(x) = \\frac{x - x_i}{x_{i+m+1} - x_i} B_i^{m-1}(x) + \\frac{x_{i+m+2} - x}{x_{i+m+2} - x_{i+1}} B_{i+1}^{m-1}(x), \\quad i=1,...k\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    B_i^{-1}(x) = \\begin{cases}\n",
    "                    1 \\quad if  \\ x_i \\le x < x_{i+1} \\\\\n",
    "                    0 \\quad else \n",
    "                  \\end{cases}\n",
    "$$\n",
    "(see e.g. de Boor 1978). \n",
    "\n",
    "B-splines were developed by de Boor as a very stable basis for large scale spline interpolation, but for most statistical work with low rank penalized regression spline, you would have to be using very poor numerical methods before the enhanced stability of the basis became noticable. The real statistical interest in B-splines has resulted from the work of Eilers and Marx (1996) in using them to develop what they term *P-splines*.\n",
    "\n",
    "P-splines are low rank smoothers using a B-spline basis, usually defined on evenly spaced knots, and a difference penalty applied directly to the parameters $\\beta_i$ to control function wiggliness. If the squared difference between adjacent $\\beta_i$ values are penalized then the penalty would be\n",
    "$$\n",
    "    \\mathcal P = \\sum_{i=1}^{k-1} (\\beta_{i+1} - \\beta_i)^2 = \\beta_1^2 - 2 \\beta_1 \\beta_2 + 2 \\beta_2^2 - 2\\beta_2 \\beta_3 + ... + \\beta_k^2,\n",
    "$$\n",
    "and it is straightforward to see that this can be written as\n",
    "$$\n",
    "    \\mathcal P = \\beta^T \\begin{bmatrix} 1 & -1 &  0& .& . \\\\\n",
    "                                        -1 &  2 & -1& .& . \\\\\n",
    "                                         0 & -1 &  2& .& . \\\\\n",
    "                                         . &  . &  .& .& . \\\\\n",
    "                                         . &  . &  .& .& .\n",
    "                         \\end{bmatrix} \\beta. \n",
    "$$\n",
    "Higher order penalties are also possible.\n",
    "\n",
    "P-splines are extremely easy to set up and use, and allow a good deal of flexibility, in that any order of penalty can be combined with any order of B-spline basis. Their disadvantage is that the simplicity is somewhat diminished if uneven knot spacing is required and that the penalties are less easy to interpret in terms of properties of fitted smooth than the more usual spline penalites. \n",
    "\n",
    "* [ ] Exercise 7\n",
    "* [ ] Exercise 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.5 Thin plate regression splines\n",
    "The bases covered so far are each useful in practice, <span style=\"background-color: #FFFF00\">but are open to some criticism:</span>\n",
    "\n",
    "1. necessary to choose knot locations, therefor extra degree of subjectivity\n",
    "2. only useful representing smooths of one predictor variable\n",
    "3. not clear to what extend this bases are better or worse than any other basis that might be used\n",
    "\n",
    "**Thin plate splines** \\\n",
    "They were introduced by Duchon (1977). Very elegant and general solution to the problem of estimating a smooth function of multiple predictor variables, from noisy observations of the function. The problem is to estimate a smooth function $g(x)$, from $n$ observations $(x_i, y_i)$ such that\n",
    "$$\n",
    "    y_i = g(x_i) + \\epsilon_i\n",
    "$$ \n",
    "where $\\epsilon_i$ is a random error term and $x \\in \\mathbb R^d$ ($d \\le n$).\n",
    "\n",
    "Thin plate spline smoothing estimates $g$ by finding the function $\\hat f$ minimizing:\n",
    "$$\n",
    "    \\lVert y - f \\rVert^2 + \\lambda J_{md}(f) \\quad (4.4)\n",
    "$$\n",
    "where $y$ is the vector of data and $f = (f(x_1), f(x_2), ..., f(x_n))^T$. $J_{md}(f)$ is a penalty function measuring the 'wiggliness' of $f$ and $\\lambda$ is a smoothing parameter. $J_{md}(f)$ is defined as\n",
    "$$\n",
    "    J_{md}(f) = \\int ... \\int_{\\mathbb R} \\sum_{\\nu_1+...+\\nu_d=m} \\frac{m!}{\\nu_1!...\\nu_d!} \\big( \\frac{\\partial^m f} {\\partial x_1^{\\nu_1}... \\partial x_d^{\\nu_d}}\\big)^2 dx_1 ... dx_2. \\quad (4.5)\n",
    "$$\n",
    "The penalty function looks somewhat intimidating, but an example will help. In the case of a smooth of two predictors with wiggliness measured using second derivatives, we have\n",
    "$$\n",
    "    J_{22} = \\int \\int \\big(\\frac{\\partial^2 f}{\\partial x_1^2}\\big)^2 +\n",
    "                       \\big(\\frac{\\partial^2 f}{\\partial x_1 x_2}\\big)^2 + \n",
    "                       \\big(\\frac{\\partial^2 f}{\\partial x_2^2}\\big)^2 dx_1 dx_2.\n",
    "$$\n",
    "Further progress is only possible if $m$ is choosen so that $2m > d$ (for 'visually smooth' results one needs $2m > d+1$. Subject to the first of these restrictions, it can be shown that the function minimizing (4.4) has the form\n",
    "$$\n",
    "    \\hat f(x) = \\sum_{i=1}^n \\delta_i \\eta_{md}(\\lVert x - x_i \\rVert) + \\sum_{j=1}^M \\alpha_j \\phi_j(X), \\quad (4.6)\n",
    "$$\n",
    "where $\\alpha$ and $\\delta$ are the coefficients to be estimated, $\\delta$ being subject to the linear constraint that $T^T \\delta = 0$ where $T_{ij} = \\phi_j(x_i)$. The $M = \\binom{m+d-1}{k}$ functions $\\phi_i$ are linearly independent polynomials spanning the space of polynomials in $\\mathbb R^d$ of degree less than $m$. For example, for $m = d = 2$ these functions are $\\phi_1(x) = 1, \\phi_2(x) = x_1$ and $\\phi_3(x) = x_2$. The remaining basis functions used in (4.6) are defined as \n",
    "$$\n",
    "    \\eta_{md}(r) = \\begin{cases}\n",
    "                            \\frac{(-1)^{m+1+d/2}}{2^{2m-1} \\pi^{d/2} (m-1)!(m-d/2)!} r^{2m-d} \\log (r) \\quad if \\ d \\ even \\\\\n",
    "                            \\frac{\\Gamma(d/2 - m)}{2^{2m} \\pi^{d/2} (m-1)!} r^{2m-d} \\quad \\quad \\quad if \\ d \\ even\n",
    "                    \\end{cases}\n",
    "$$\n",
    "\n",
    "Now defining the matrix $E$ by $E_{ij} = \\eta_{md}(\\lVert x_i - x_j \\rVert)$, the thin plate spline fitting problem becomes, \n",
    "$$\n",
    "    \\min_{\\alpha, \\delta} \\lVert y - E \\delta - T \\alpha \\rVert^2 +  \\lambda \\delta^T E \\delta \\ s.t. \\ T^T \\delta = 0, \\quad (4.7)\n",
    "$$\n",
    "The thin plate spline $\\hat f$ is something of an ideal smoother: it has been constructed by defining\n",
    "\n",
    "- what is meant by smoothness\n",
    "- how much weight to give the conflicting goal of matching the data and making $\\hat f$ smooth\n",
    "- finding the function that best satisfies the resulting smoothing objective. \n",
    "\n",
    "One did not need to choose knot positions or select basis functions. In addition, TPS can deal with an arbitrary number of predictors. Here also lies the problem of TPS: the computational cost is proportional to the cube of the number of parameters (which is the number of data $n$). Given that the effective degrees of freedom estimated for a model term is usually a small proportion of $n$, the question arises whether a low rank approximation could be produced which is as close as possible to the TPS, without incurring the high computational cost.\n",
    "\n",
    "**Thin plate regression splines** \\\n",
    "Are based on the idea of truncating the space of wiggly componets of the TPS (with parameters $\\delta$), while leaving the components of 'zero wiggliness' unchanged (with parameters $\\alpha$). This is done by an eigen-decomposition of the matrix $E$ into $E = UDU^T$. Now only take the first $k$ columns of $U$ and denote them $U_k$. $D_K$ denotes the top right $k \\times k$ submatrix of $D$. Restricting $\\delta$ to the columns space of $U_k$, by writing $\\delta = U_k \\delta_k$, means that (4.7) becomes \n",
    "$$\n",
    "    \\min_{\\alpha, \\delta_k} \\lVert y - U_k D_k \\delta_k - T \\alpha \\rVert^2 +  \\lambda \\delta_k^T D_k \\delta_k \\ s.t. \\ T^U_k \\delta_k = 0.\n",
    "$$\n",
    "The constraints can be absorbed in the usual manner, described in Section 1.8.1 (General linear constraints) and A.6 (QR-decomposition).\n",
    "\n",
    "First we find any orthogonal column basis $Z_k$, such that $T^T U_k Z_k = 0$ (e.g. by $QR$ decomposition of $U_k^T T$: the final $M$ columns of the orthogonal factor give a $Z_k$). Restricting the $\\delta_k$ to this space ( $\\delta_k = Z_k \\tilde \\delta$), yields the unconstrained problem that must be solved to fit the rank $k$ approximation to the smoothing spline:\n",
    "$$\n",
    "    \\min_{\\alpha, \\tilde \\delta} \\lVert y - U_k D_k Z_k \\tilde \\delta - T \\alpha \\rVert^2 +  \\lambda \\tilde \\delta^T Z_k^T D_k Z_k \\tilde \\delta \n",
    "$$\n",
    "This has a computational cost of $\\mathcal O(k^3)$. Having the fitted model, evaluation of the spline at any point is easy: just evaluate $\\delta = U_K Z_K \\tilde \\delta$ and use (4.6). The eigen-decomposition can be found by Lanczos iteration ($\\mathcal O(n^2k)$ )for large systems and be full $QR$ ($\\mathcal O(n^3)$) for small systems. \n",
    "\n",
    "**Properties of TPRS** \\\n",
    "TPRS properties are:\n",
    "\n",
    "- avoid knot placement\n",
    "- relatively cheap to compute\n",
    "- can use any number of predictor variables\n",
    "- [ ] what about optimality ? \n",
    "\n",
    "**Knot based approximation** \\\n",
    "If knot locations $\\{x_i^*: i=1...k\\}$ are chosen, then the spline can be approximated by\n",
    "$$\n",
    "    \\hat f(x) = \\sum_{i=1}^k \\delta_i \\eta_{md}(\\lVert x - x_i^*\\rVert) + \\sum_{j=1}^M \\alpha_j \\phi_j(x), \\quad (4.8)\n",
    "$$\n",
    "where $\\delta$ and $\\alpha$ are estimated by minimizing\n",
    "$$\n",
    "    \\lVert y - X \\beta \\rVert^2 + \\lambda \\beta^T S \\beta, \\quad \\ s.t. \\ C\\beta = 0\n",
    "$$\n",
    "w.r.t $\\beta^T = (\\delta^T, \\alpha^T)$. $X$ is an $n \\times k + M$ matrix such that\n",
    "$$\n",
    "    X_{ij} = \\begin{cases}\n",
    "                \\eta_{md} (\\lVert x_i - x_j^* \\rVert ) \\quad j=1, ..., k \\\\\n",
    "                \\phi_{j-k}(x_i) \\quad j=k+1, ..., k + M,\n",
    "             \\end{cases}\n",
    "$$\n",
    "\n",
    "$S$ is a $(k + M) \\times (k + M)$ matrix with zeros everywhere except in its upper left $k \\times k$ block where $S_{ij} = \\eta_{md}(\\lVert x_i^* - x_j^* \\rVert)$. Finally, $C$ is an $M \\times (k + M)$ matrix such that \n",
    "$$\n",
    "    C_{ij} = \\begin{cases}\n",
    "                \\phi_i(x_j^*) \\quad j=1, ..., k \\\\\n",
    "                0 \\quad j = k+1, ..., k+M.\n",
    "             \\end{cases}\n",
    "$$\n",
    "This approximation goes back at least to Wahba (1980). Some care is required to choose the knot locations. For more dimensions it is often difficult. One possibility is to take a random sample of observed predictor variable combinations, another is to take a 'spatially stratified' sample of the predictor variable combinations. Even spaceing is sometimes appropriate, or more sophisticated space filling schemes can be used: Ruppert et al. (2003) provided a useful discussion of the alternatives. \n",
    "\n",
    "- [ ] Section 1.8.1: Constraint - absorbtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.6 Shrinkage smoothers\n",
    "\n",
    "A disadvantage of the smooths discussed so far, is that no matter how large their associated smoothing parameter becomes, the smooth is never completely eliminated in the sense of having all its parameters estimated to be zero. On the contrary, some functions are treated as completely smooth by the penalty, and hence functions of this class are always completely un-penalized. From the point of view of *model selection* with GAMs it would be more convenient if smooths could be zeroed by adjustment of smoothing parameters. \n",
    "\n",
    "A fairly crude alternative, is simply to add a small multiple of the identity matrix to the penalty matrix of the smooth, i.e. \n",
    "$$\n",
    "    S \\rightarrow S + \\epsilon I\n",
    "$$\n",
    "so that the penalty will now shrink all parameters to zero if its associated smoothing parameter is large enough. If $\\epsilon$ is small enough, the identity part of the penalty\n",
    "will have almost no impact when a function is â€˜wigglyâ€™: only once it becomes close\n",
    "to â€˜completely smoothâ€™ will the identity component start to become important, and\n",
    "really start shrinking the parameters towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.7 Choosing the basis dimension\n",
    "\n",
    "When using penalized regression splines the modeller chooses the basis dimension as part of the model building process. Typically, this substantially reduces the computational burden of modelling, relative to full spline methods, and recognizes the fact that, usually, something is seriously wrong if a statistical model really requires as many coefficients as there are data. \n",
    "\n",
    "The main challenge introduced, by this low rank approach, is that a basis dimension has to be chosen. In the context of spline smoothing, Kim and Gu (2004) showed that the basis size should scale as $n^{2/9}$, where $n$ is the number of data.\n",
    "\n",
    "In practice, then, choice of basis dimension is something that probably has to remain a part of model specification. However, it is important to note that the exact size of basis dimension is really not that critical. The basis dimension is only setting an upper bound on the flexibility of a term: it is the smoothing parameter that controls the actual effective degrees of freedom. Hence the model fit is usually rather insensitive to the basis dimension, provided that it is not set restrictively low for the application concerned. \n",
    "\n",
    "<span style=\"background-color: #FFFF00\"> The only caveat to this point is the slightly subtle one, that a function space with basis dimension 20 will contain a larger space of functions with EDF 5 than will a function space of dimension 10 (the numbers being arbitrary): it is this fact that causes model fit to retain some sensitivity to basis dimension, even if the appropriate EDF for a term is well below the basis dimension.In practice, the modeller needs to decide roughly how large a basis dimension is fairly certain to provide adequate flexibility, in any particular application, and use that. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.8 Tensor product smooths\n",
    "\n",
    "A major feature of the TPS/TPRS approach is the isotropy of the wiggliness penalty: wiggliness in all directions is treated equally, with the fitted spline entirely invariant to rotation of the co-ordinate system for the predictor variables. This isotropy is often considered to be desirable (especially as a smooth function of geographic co-ordinates), but comes with some disadvantages, e.g. the difficulty of knowing how to scale predictors relative to each other, when they are measured in fundamentally different units. One pragmatic approach is to scale all predictors into the unit square. A more satisfactory approach uses *tensor product smooths*. \n",
    "\n",
    "**Tensor product bases** \\\n",
    "The basic approach of this section is to start from smooths of single covariates, represented using any basis with associated quadratic penalty measuring 'wiggliness' of the smooth. From these <span style=\"background-color: #FFFF00\">'marginal smooths' a  tensor product construction</span>  is used to build up smooths of several variables. See de Boor (1978) for an important early reference on tensor product spline bases.\n",
    "\n",
    "As example, a smooth function of 3 covariates $x, z$ and $v$ is constructed (in general this can be done with an arbitrary number of covariates). The process starts by assuming that we have low rank bases available, representing smooth functions $f_x$, $f_z$ and $f_v$. That is\n",
    "$$\n",
    "    f_x(x) = \\sum_{i=1}^I \\alpha_i a_i(x), \\quad \n",
    "    f_z(z) = \\sum_{l=1}^L \\delta_l d_i(z), \\quad\n",
    "    f_v(v) = \\sum_{k=1}^K \\beta_k b_i(x)\n",
    "$$\n",
    "where $\\alpha_i$, $\\delta_l$ and $\\beta_k$ are parameters and $a_i(x)$, $d_l(z)$ and $b_k(v)$ are known basis functions.  \n",
    "\n",
    "What is required for $f_x$ to vary smoothly with $z$? This can be achieved by allowing its parameters $\\alpha_i$ to vary smoothly with $z$. Using the already known basis we could write\n",
    "$$\n",
    "    \\alpha_i(z) = \\sum_{l=1}^L \\delta_{il} d_l(z)\n",
    "$$\n",
    "which immediately gives\n",
    "$$\n",
    "    f_{xz}(x, z) = \\sum_{i=1}^I \\sum_{l=1}^L \\delta_{il} d_l(z) a_i(x).\n",
    "$$\n",
    "\n",
    "Inthe same way, a smooth function of any number of covariates can be construted. In our example, this leads then to\n",
    "$$\n",
    "    f_{xzv}(x,z,v) = \\sum_{i=1}^I \\sum_{l=1}^L \\sum_{k=1}^K \\beta_{ilk} b_k(v) d_l(z) a_i(x) \n",
    "$$\n",
    "The model matrix $X$ can be constructed by using the Kronecker product, given an appropriate ordering of the $\\beta_{ijl}$ into a vector $\\beta$. The $i^{th}$ row of $X$ is simply\n",
    "$$\n",
    "    X_i = X_{xi} \\otimes X_{zi} \\otimes X_{vi}.\n",
    "$$\n",
    "From this follows:\n",
    "\n",
    "- the construction can be done for an arbitrary number of covariates\n",
    "- the results are independent of the order in which the covariates are treated\n",
    "- the covariates can themselves be vector covariates\n",
    "\n",
    "**Tensor product penalties** \\\n",
    "Suppose each marginal smooth has an associated functional measuring function wiggliness which can be expressed as a quadratic form in the marginal parameters, like\n",
    "$$\n",
    "    J_x(f_x) = \\alpha^T S_x \\alpha, \\quad\n",
    "    J_z(f_z) = \\delta^T S_z \\delta, \\quad\n",
    "    J_v(f_v) = \\beta^T S_v \\beta.\n",
    "$$\n",
    "The $S_*$ matrices contain known coefficients, and the $\\alpha$, $\\delta$ and $\\beta$ are vectors of coefficients of the marginal smooths. Each marginal penalty is the associated with a smoothing parameter $\\lambda_*$. Hence, if the marginal penalties are easily interpretable, in terms of function shape (e.g. the cibuc spline penalty), then so is the induced penalty. As an example, if cubic spline penalties were used as the marginal penalties, then\n",
    "$$\n",
    "    J(f) = \\int_{x,z,v} \\lambda_x \\big( \\frac{\\partial^2 f}{\\partial x^2}\\big) + \n",
    "                        \\lambda_z \\big( \\frac{\\partial^2 f}{\\partial z^2}\\big) +\n",
    "                        \\lambda_v \\big( \\frac{\\partial^2 f}{\\partial v^2}\\big) dx dz dv.\n",
    "$$\n",
    "The integral can be performed numerically, and it is clear that the same approach can be applied to all components of the penalty. However, a simple reparameterization (p. 162f) can be used to provide an approximation to the terms in the penalty, which performs well in practice, and avoids the need for explicit numerical integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import lstsq, inv\n",
    "from scipy.sparse import diags, csr_matrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def addVertLinePlotly(fig, x0=0, y0=0, y1=1):\n",
    "    \"\"\" plots a vertical line to the given figure at position x\"\"\"\n",
    "    fig.add_shape(dict(type=\"line\", x0=x0, x1=x0, y0=y0, y1=1.2*y1, \n",
    "                       line=dict(color=\"LightSeaGreen\", width=1)))\n",
    "    return\n",
    "\n",
    "#######################################################################\n",
    "# trusted\n",
    "#######################################################################\n",
    "# BSpline functionality    \n",
    "#######################################################################\n",
    "def bSpline(x, k, i, m=2):\n",
    "    \"\"\" \n",
    "    evaluate i-th b-spline basis function of order m at the \n",
    "    values in x, given knot loactions in k\n",
    "    \"\"\"\n",
    "    if m==-1:\n",
    "        # return 1 if x is in {k[i], k[i+1]}, otherwise 0\n",
    "        return (x >= k[i]) & (x < k[i+1]).astype(int)\n",
    "    else:\n",
    "        #print(\"m = \", m, \"\\t i = \", i)\n",
    "        z0 = (x - k[i]) / (k[i+m+1] - k[i])\n",
    "        z1 = (k[i+m+2] - x) / (k[i+m+2] - k[i+1])\n",
    "        return z0*bSpline(x, k, i, m-1) + z1*bSpline(x, k, i+1, m-1)\n",
    "\n",
    "# trusted\n",
    "def modelMatrixBSpline(x, k=0, m=2):\n",
    "    \"\"\" \n",
    "    set up model matrix for the B-spline basis\n",
    "    one needs k + m + 1 knots for a spline basis of order m with k \n",
    "    parameters,\n",
    "    k - number of parameters (== number of B-splines)\n",
    "    \"\"\"\n",
    "    n = len(x) # n = number of data\n",
    "    assert (k > 0), \"number of B-splines k need to be specified and larger than 0\"\n",
    "    xmin, xmax = np.min(x), np.max(x)\n",
    "    xk = np.quantile(a=x, q=np.linspace(0,1,k))\n",
    "    dx = xk[m+3] - xk[m+2]\n",
    "    xk = np.insert(xk, 0, np.arange(xmin-(m+1)*dx, xmin, dx))\n",
    "    xk = np.append(xk, np.arange(xmax+dx, xmax+(m+2)*dx, dx))\n",
    "    X = np.zeros(shape=(n, k))\n",
    "    for i in range(k):\n",
    "        X[:,i] = bSpline(x=x, k=xk, i=i+1, m=m)\n",
    "    return X, xk\n",
    "\n",
    "#trusted\n",
    "def pSplinePenalty(k):\n",
    "    \"\"\" \n",
    "    compute the P-spline second order difference penalty matrix of \n",
    "    dimension k according to p.150 Wood 2006 \n",
    "    \"\"\"\n",
    "    assert (type(k) is int), \"Type of input k is not int\"\n",
    "    a =np.eye(k, dtype=np.int)\n",
    "    P = np.diff(a, axis=0)\n",
    "    S = P.T @ P\n",
    "    return S\n",
    "\n",
    "# trusted\n",
    "def plot_ModelMatrixBSpline(x, k, m=2):\n",
    "    X, xk = modelMatrixBSpline(x, k=k, m=m)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i in range(X.shape[1]):\n",
    "        fig.add_trace(go.Scatter(x=x, y=X[:,i], \n",
    "                                 name=f\"BSpline {i+1}\", mode=\"lines\"))\n",
    "    for i in xk:\n",
    "        addVertLinePlotly(fig, x0=i)\n",
    "    fig.show()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peak_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-94f137e23b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpeak_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlam_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mmse_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfit_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'peak_fit' is not defined"
     ]
    }
   ],
   "source": [
    "k = 75\n",
    "\n",
    "mse_list = list()\n",
    "fit_list = list()\n",
    "lam_p = 0.01\n",
    "for i in range(10):\n",
    "    \n",
    "    fit = peak_fit(x_train, y_train, k=k, lam_w=1, lam_p=lam_p, plot_=0,x_test=x_test)\n",
    "    mse_list.append(np.round(mse(y_test, fit[1]), 6))\n",
    "    fit_list.append(fit[0])\n",
    "    lam_p *= 2.5\n",
    "    print(\"Lambda peak: \", lam_p)\n",
    "    \n",
    "print(\"Mean Squared Errors: \", mse_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
