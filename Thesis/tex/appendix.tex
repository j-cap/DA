\begin{appendices}
	
\chapter{Appendix} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kronecker product} \label{apx:AppendixKronecker}

The Kronecker product $\vec{A} \otimes \vec{B}$ of the $n \times p$-matrix $\vec{A}$ and the $r \times q$-matrix $\vec{B}$ is defined as $nr \times pq$-matrix 

\begin{align}
	\vec{C} = \vec{A} \otimes \vec{B} = 
		\begin{bmatrix}
			a_{11} \vec{B} & \dots & a_{1p} \vec{B} \\
			\vdots 	       &       &  \vdots \\
			a_{n1} \vec{B} & \dots & a_{np} \vec{B}     
		\end{bmatrix}. 
\end{align}
%
As example, we define $\vec{A} \in \mathbb{R}^{3 \times 2}$ and $\vec{B} \in \mathbb{R}^{2 \times 2}$ as

\begin{align}
	\vec{A} = \begin{bmatrix}
			1 & 2  \\
			3 & 4 \\
			5 & 6		
		\end{bmatrix}, \quad 
	\vec{B} = 
		\begin{bmatrix}
			1 & 2  \\
			1 & 2   
		\end{bmatrix},
\end{align}
%
resulting in the matrix $\vec{C} \in \mathbb{R}^{6 \times 4}$ given by

\begin{align}
	\vec{C} = 
	\begin{bmatrix}
		a_{11} \vec{B} &  a_{12} \vec{B} \\
		a_{21} \vec{B} &  a_{22} \vec{B}
	\end{bmatrix} = 
	\begin{bmatrix}
		1 & 2 & 2 & 4  \\
		1 & 2 & 2 & 4 \\
		3 & 6 & 4 & 8 \\
		3 & 6 & 4 & 8 \\
		5 & 10& 6 &12 \\
		5 & 10& 6 &12
	\end{bmatrix}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Row-wise Kronecker product} \label{apx:AppendixKroneckerRowWise}

Given the $n \times p$-matrix$\vec{A}$ and the $n \times q$-matrix $\vec{B}$, the row-wise Kronecker product is defined as $n \times pq$-matrix $\vec{C}$ given by

\begin{align}
	\vec{C} = \vec{A} \odot \vec{B} = 
			\begin{bmatrix}
				a_{11} \vec{b}_1 & \dots & a_{1p} \vec{b}_1 \\
				\vdots 	   &         &  \vdots \\
				a_{n1} \vec{b}_n & \dots  & a_{np} \vec{b}_n     
			\end{bmatrix},
\end{align}
%
where $\vec{b}_i$ denotes the $i$-th row of the matrix $\vec{B}$. As example, we define $\vec{A} \in \mathbb{R}^{3 \times 3}$ and $\vec{B} \in \mathbb{R}^{3 \times 2}$ as

\begin{align}
	\vec{A} = 
		\begin{bmatrix}
			\vec{a}_1 \\
			\vec{a}_2 \\
			\vec{a}_3 
		\end{bmatrix} &= 
		\begin{bmatrix}
			1 & 2 & 3 \\
			4 & 5 & 6 \\
			7 & 8 & 9 
		\end{bmatrix}, \quad 
	\vec{B} = 
		\begin{bmatrix}
			\vec{b}_1 \\
			\vec{b}_2 \\
			\vec{b}_3 
		\end{bmatrix} = 
		\begin{bmatrix}
			1 & 2  \\
			1 & 2  \\
			1 & 2  
		\end{bmatrix},
\end{align}
%
where $\vec{a}_i$ denotes the $i$-th row of the matrix $\vec{A}$ resulting in the matrix $\vec{C} \in \mathbb{R}^{3 \times 6}$ given by

\begin{align}
		\vec{C} = 
	\begin{bmatrix}
		\vec{a}_1 \otimes \vec{b}_1 \\
		\vec{a}_2 \otimes \vec{b}_2 \\
		\vec{a}_3 \otimes \vec{b}_3 
	\end{bmatrix} = 
	\begin{bmatrix}
		1 & 2 & 2 & 4 & 3 & 6 \\
		4 & 8 & 5 &10 & 6 & 12\\
		7 & 14& 8 & 16& 9 & 19 
	\end{bmatrix}.
\end{align}
%
The row-wise Kronecker product is also known as \emph{face-splitting product} or as \emph{transposed Khatri-Rao product} \cite{slyusar1997analytical}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Derivation of the iterative scheme} \label{apx:AppendixB}

The following derivation is take from~\cite{bollaerts2006simple}. To find an optimal solution for the penalized least squares objective function for shape-constraint P-splines given by

\begin{align} \label{eq:L}
	\text{L}(\vec{\beta}) = \lVert \vec{y} - \vec{X} \vec{\beta} \rVert_2^2 + \lambda \transpose{\vec{\beta}} \vec{K} \vec{\beta} + \lambda_c \transpose{\vec{\beta}} \vec{K}_c \vec{\beta},
\end{align}
%
we use a Newton-Raphson scheme. At each iteration $i$, the new estimate $\hat{\vec{\beta}}_{i+1}$ is computed such that

\begin{align} \label{eq:NR-scheme}
	\vec{g}(\vec{\beta}_{i}) + \vec{H}(\vec{\beta}_{i}) \big( \vec{\beta}_{i+1} - \vec{\beta}_{i}\big) = 0,
\end{align}
%
with $\vec{g}$ being the gradient and $\vec{H}$ being the Hessian of the objective function $\text{L}(\vec{\beta})$. The gradient of $\text{L}(\vec{\beta})$ equals to

\begin{align}
	\vec{g}(\vec{\beta}) = -2\transpose{\vec{X}} \vec{y} + 2\Big[ \transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}) \vec{D}_c\Big] \vec{\beta} + \lambda_c \transpose{\vec{\beta}} \transpose{\vec{D}}_c \frac{\partial \vec{V}_c(\vec{\beta})}{\partial \vec{\beta}} \vec{D}_c \vec{\beta},
\end{align}
%
which can be simplified, see~\cite{bollaerts2006simple}, to

\begin{align} \label{eq:gradient-of-L}
	\vec{g}(\vec{\beta}) = -2\transpose{\vec{X}} \vec{y} + 2\Big[ \transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}) \vec{D}_c\Big] \vec{\beta}. 
\end{align}
%
The gradient is therefore a piecewise linear function of $\vec{\beta}$ since $\vec{V}(\vec{\beta})$ is a diagonal matrix with 0s and 1s as diagonal elements, see~\pref{eq:gradient-of-L}. This implies that the Hessian is a step function of $\vec{\beta}$ and equal to

\begin{align} \label{eq:hessian-of-L}
	\vec{H}(\vec{\beta}) = 2\transpose{\vec{X}} \vec{X} + 2\lambda \transpose{\vec{D}}_2 \vec{D}_2 + 2\lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}) \vec{D}_c + 2\lambda_c \transpose{\vec{D}}_c \frac{\partial \vec{V}_c(\vec{\beta})}{\partial \vec{\beta}} \vec{D}_c. 
\end{align}
%
The last part can be neglected again, see \cite{bollaerts2006simple}. Substituting~\pref{eq:gradient-of-L} and~\pref{eq:hessian-of-L} into~\pref{eq:NR-scheme} leads to

\begin{align}
	\begin{split}
	0 ={}& -2\transpose{\vec{X}} \vec{y} \\ 
	   {}&+ 2\Big[ \transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}_i) \vec{D}_c\Big] \vec{\beta}_i \\ 
	   {}&+ 2\Big[\transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}_i) \vec{D}_c \Big] \big( \vec{\beta}_{i+1} - \vec{\beta}_i \big),
	\end{split}
\end{align}
%
which can be reformulated as

\begin{align}
	-\transpose{\vec{X}} \vec{y} + \Big[ \transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}_i) \vec{D}_c \Big] \vec{\beta}_{i+1} = 0,
\end{align}
%
and, hence, we obtain

\begin{align}
	\vec{\beta}_{i+1} = \Big[ \transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}_c(\vec{\beta}_i) \vec{D}_c \Big]^{-1} \transpose{\vec{X}} \vec{y}.
\end{align}

\end{appendices}