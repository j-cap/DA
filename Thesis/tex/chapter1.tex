\chapter{Introduction} \label{cha:introduction}

The digitalization of complex, large-scale industrial plants is leading to a massive increase of process data. This data can be used to enhance the overall understanding of the characterizing physical process inside the plant. Modern observer and control concepts are used to enhance the efficiency and quality  of the plant. They use mathematical models of the ongoing process. The exact physical description of the relevant quantities as mathematical model is nevertheless often not feasible because of the complexity of the process as well as computational and measurement limitations.

Data driven approaches are state-of-the-art in many fields, including e.g. image and speech recognition. The usage of data driven methods, e.g. artificial neural networks, parametric models, etc., to model process quantities for which measurements are expensive or not practical, gains more and more influence and acceptance in the field of process control and optimization. The application of any specific algorithm depends on issues like data quality, interpretability of the model and computational efficiency. 

In most process optimization tasks massive amounts of domain specific knowledge in form of physical theories and a priori knowledge is available. The combination of the use of domain knowledge and data driven modeling techniques is called hybrid modeling or grey-box modeling. It lies between the two modeling extrema of white-box models, which are derived from first principles and physical models and black-box models, which are derived from data only \cite{ashby1961introduction}. The incorporation of this knowledge in state-of-the-art data driven approaches is not trivial and not solved for some algorithms. Nevertheless, its inclusion should improve the interpretability, which itself is of importance in the context of the emerging field of explainable artificial intelligence (XAI). XAI refers to modeling approaches and techniques in which the main goal is that the resulting model is understandable by humans \cite{dovsilovic2018explainable}.

In this thesis, we present an algorithm for efficient, static, multi-dimensional function approximation using a priori domain knowledge. The algorithm is based on structured additive regression using B-splines \cite{fahrmeir2007regression}. We use user-defined constraints to include the a priori domain knowledge in the fitting process, see \cite{hofner2011monotonicity} and \cite{bollaerts2006simple}. We produce interpretable and efficient models based on the given domain knowledge. The incorporation of domain specific knowledge improves the model quality and robustness as well as the interpolation behavior in situations where the measured data is sparse and/or noisy. Further, we evaluate the algorithm using noisy samples from artificial test functions with known behavior as well as real world data collected in a heat treatment process. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

We will now discuss commonly used data-driven algorithms. The discussion includes the following model approaches

\begin{itemize}
	\item Parametric models: Linear and polynomial regression
	\item Non-parametric models: Basis function models
	\item Gaussian process regression
	\item Artificial neural networks
	\item Look-up tables
\end{itemize}
%
and focus on the interpretability, computational efficiency and the ability to include domain knowledge of the individual modeling approaches. The list given above is not intended to give a complete overview of the field of data-driven modelling but rather to be an introduction.

The common starting point for the different data-driven modeling approaches is that we have some data $\mathcal{D}$, i.e.

\begin{align}
	\mathcal{D} = \left\{ (x_1^{(i)}, \dots, x_q^{(i)}; y^{(i)} ), \ i = 1, \dots, n\right\}.
\end{align} 
%
For ease of presentation, we restrict the following discussion to the single-input setting, i.e. $(x^{(i)}, y^{(i)}), \ i=1, \dots, n$. The generalization to multiple input dimensions is given in the respective literature. We then use the given data to estimate a model function $f(x)$  which is then used to predict the response or output variable $y$, i.e.

\begin{align} \label{eq:basic-model-structure}
	y = f(x).
\end{align}
%
Therefore, we are in the setting of supervised learning. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parametric Models}

According to \cite{nelles2013nonlinear}, parametric models are defined as models that can describe the true process behavior using a finite number of parameters. An example is given by the linear regression model for one input variable $x$ as

\begin{align} \label{eq:classical-linear-model}
	y= f(x) = \beta_0 + \beta_1 x.
\end{align}
%
Both parameters $\beta_0$ and $\beta_1$ allow for a direct interpretation as $\beta_0$ is the intercept, i.e. the output for the input $x=0$, and $\beta_1$ is the slope, i.e. the constant defining the relationship between the increase of the output $y$ with respect to the increase of the input $x$.  The interpretability of linear regression models is therefore very high. 

Linear regression models are widely used and part of standard software tools. Their parameters can be efficiently computed using the least squares algorithm. One major drawback is that they can only recover linear relationships between input and output variables. They are therefore quite restrictive and do not allow the incorporation of a priori domain knowledge except being increasing or decreasing by the sign of the slope $\beta_1$. 

An extension of the linear regression model is given by polynomial regression. Here, we try to model the output data $y$ using a polynomial of degree $p$, i.e.

\begin{align} \label{eq:polynomial-model}
	y = f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_p x^p.
\end{align}
%
Polynomial regression introduces more flexibility in the fitting process, since the restriction of linear relationship is relaxed to a polynomial relationship of degree $p$. As for linear models, the interpretability of the parameters is given. We can use the least squares algorithm for parameter estimation. The incorporation of some a priori domain knowledge is possible, e.g. as the degree of the polynomial regression model. The major problem of polynomial regression is that the model function becomes quite wiggly for high polynomial degrees $p$. 

Linear and polynomial regression models are so-called global models. Their parameters act on the complete input space. This property makes the incorporation of specific a priori domain knowledge, e.g. unimodal behavior, difficult and in most cases nearly impossible for parametric models. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Non-parametric Models}

In \cite{nelles2013nonlinear}, non-parametric models are defined as models which require an infinite number of parameters to describe a process exactly. In almost all practical applications this infinite series is approximated by a finite number of parameters using the basis function approach given by

\begin{align} \label{eq:basis-function-approach}
	y = f(x) = \sum_{i=1}^M \beta_i \Phi_i(x, \theta_i),
\end{align} 
%
with the parameters $\beta_i$, the basis functions $\Phi_i(\cdot)$, the input variable $x$ and the basis function parameters $\theta_i$. The output $y$ is therefore given by a linear combination of $M$ basis functions $\Phi_i(\cdot)$. To model a nonlinear relationship between $y$ and $x$, the basis functions $\Phi(\cdot)$ need to be nonlinear. Commonly used basis functions are, e.g. the \emph{hat function}, the \emph{Gaussian}, \emph{splines} or the \emph{hinge function} \cite{friedman2001elements}. 

A commonly used algorithm utilizing the basis function approach is called Multivariate Adaptive Regression Splines (MARS) \cite{friedman1991multivariate}. MARS approximates data, as example again for a single input dimension, using the following model

\begin{align} \label{eq:MARS}
	y = \sum_{i=1}^M \theta_i \Phi_i(x)
\end{align}
%
using constant parameters $\theta_i$. The basis functions $\Phi_i(\cdot)$ are one of the following three alternatives:

\begin{enumerate}
	\item $\Phi_i(x) = 1$, representing the intercept.
	\item $\Phi_i(x) = \max(0, x - c_i)$ or $= \max(0, c_i - x)$, representing the \emph{hinge function} $h_i$ using the constant value $c_i$.
	\item $\Phi_i(x) = h_i  h_j$, representing a product of two \emph{hinge functions}.
\end{enumerate}
%
MARS fits the model using a recursive splitting approach. More information can be found in \cite{friedman2001elements} and \cite{friedman1991multivariate}. MARS models are more flexible compared to the parametric linear and polynomial regression models. As only hinge functions and products of hinge functions are used, MARS models are efficient and in general simple to understand and interpret. To our knowledge, there is currently no possibility to include a priori domain knowledge in the fitting process when using MARS. 

Another widely used methods using basis functions is the use of \emph{splines}. Splines are defined as piece-wise polynomials on a sequence of knots. Further information can be found in Section~\ref{sec:Splines} and \cite{deBoor1978practicalGuideToSplines}. We focus on the so-called B-splines or basis-splines. Using a B-spline consisting of $d$ B-spline basis functions of order $l$ to model some data, we obtain the model formulation as

\begin{align} \label{eq:Spline-basis-formulation}
	y = f(x) = \sum_{j=1}^d B_j^l(x) \beta_j, 
\end{align}
%
with the B-spline basis functions $B_j^l$ and the parameters $\beta_j$. The parameters can be calculated used the least squares algorithm. The usage of splines allows a lot of flexibility and computational efficiency. A priori domain knowledge can be incorporated using an iterative variante of the penalized least squares algorithm with a sophisticated choice of mapping and weighting matrices, see~\pref{cha:solution-approach}, and, e.g. \cite{hofner2011monotonicity} or \cite{bollaerts2006simple}.

The basis function approach in (\ref{eq:basis-function-approach}) may be extended by changing the parameters $\beta_i$ to more complex forms. An example for this is the so-called local linear neuro-fuzzy model, for which each parameter $\beta_i$ is changed to be a \emph{local linear model} and each basis function $\Phi_i(\cdot)$ is then called \emph{validity function} determining the region of validity of the local linear model \cite{nelles2013nonlinear}. The validity functions are normalized for any model input $x$, i.e.

\begin{align} \label{eq:LILOMOT-normalized-basis-fucntions}
	\sum_{i=1}^M \Phi_i(x) = 1
\end{align}
%
and typically chosen to be \emph{Gaussian} functions, i.e. 

\begin{align} \label{eq:validity-function}
	\Phi_i(x) = a_i \exp \left(\frac{(x - \mu_i)^2}{\sigma_i^2} \right),	
\end{align}
%
with the normalization constant $a_i$ and the parameters $\mu_i$ and $\sigma_i$ determining the location and scale of the Gaussian function. The output of the local linear neuro-fuzzy model using $M$ local linear models is then given by

\begin{align} \label{eq:LOLIMOT}
	y = \sum_{i=1}^M \left(\beta_{i0} + \beta_{i1} x_1\right) \Phi_i(x).
\end{align}
%
The first term in the summation are the \emph{local linear models}. The parameters $\beta_{ij}$ for $i=1, \dots, M$ and $j=0, 1$ as well as the parameters $\mu_i$ and $\sigma_i$ from the validity functions $\Phi_i$ need to be optimized. This is done using the LOLIMOT algorithm. Further information is given in \cite{nelles2013nonlinear}. 

Local linear models as extension of linear models possess more flexibility with regards to nonlinear relationships in the data. They can also be efficiently evaluated after the iterative training process. The interpretability is high since each local linear model contributes to the prediction according to its validity function. The ability to include a priori domain knowledge in the fitting process is currently not available.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussian Process Regression}

A Gaussian process is formally defined as a collection of random variables, any finite number of which have a joint Gaussian distribution, see \cite{rasmussen2005GPforML}. To specify a Gaussian process, we need a mean function $m(x)$ and a covariance function $k(x, x')$, defined as

\begin{align}
	m(x) = \text{E}[f(x)], \\
	k(x,x') = \text{E}\left[ (f(x) - m(x))(f(x') - m(x')) \right],
\end{align}
% 
and obtain the Gaussian process as

\begin{align}
	f(x) \approx \mathcal{GP}(m(x), k(x,x')).
\end{align}


A Gaussian process is defined as generalization of the Gaussian distribution to multiple dimensions. The mean 
There are two ways of interpreting Gaussian process regression models: the \emph{weight-space} and the \emph{function space} view. We will not go into detail here, since there are various textbooks related to this topic, see for example \cite{rasmussen2005GPforML}.





Gaussian process regression tries to reconstruct the underlying true function $f$ by removing the noise $\epsilon$ by computing an weighted average of the noisy observations $y = f(x) + \epsilon$ as

\begin{align}
	f(x) = \transpose(k(x)) (\vec{K} + \sigma_n^2 \vec{I})^{-1} y,
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Artificial Neural Networks}

Artificial neural networks are currently the state-of-the-art solution method for many problems ranging from computer vision over time-series prediction to regression tasks. They are constructed as coarse model of the human brain, consisting of neurons which are connected by some weights. These connections are adapted in the learning process using an algorithm called "backpropagation". They utilize a high number of parameters to model high-dimensional relationships in the data. Further information can be found in standard textbook about neural networks, e.g. \cite{bishop2006patternRecognition} or \cite{goodfellow2016deep}. 

In terms of modeling flexibility, artificial neural networks of sufficient size are proven to be able to represent a wide variety of functions by so-called universal approximation theorems, cf. \cite{cybenko1989approximation} and \cite{hornik1991approximation}. The computational complexity of a neural network depends on its size, aka. the number of parameters. Large networks need many training samples to generate sufficiently accurate predictions. Artificial neural networks are an example of a black-box model. The inclusion of a priori domain knowledge into the learning process of neural networks is possible for specific types of knowledge using the concepts of hints, see \cite{abu1990learning} and \cite{sill1997monotonicity}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Look-up Tables}

A look-up table is an array of values, which allows to replace computational expensive computations with inexpensive array indexing operations. The values in the look-up table are most often computed and stored beforehand. To gain higher resolution, interpolation techniques such as linear or quadratic interpolation may be applied to look-up tables. 

Look-up tables are a standard tool in many fields. They are extremely efficient in terms of computation time. One problem that occurs is the exponential increase in size with the number of dimensions for the look-up table. As example, a $2 \times 2$-table needs to save 4 values, while a $2 \times 2 \times 2$ table already needs 8 values without gaining additional accuracy. Another problem is that the values in the look-up table may come from complex, computational or physical models. 

Lattice regression tackles this problems by jointly estimating all lookup-table values by minimizing the regularized interpolation error on training data \cite{garcia2009lattice}. They state that using ensembles of lookup-tables which combine several \emph{tiny} lattices enables linear scaling in the number of input dimension even for high dimensions \cite{fard2016fast}. They further state that lattice regression may be used to incorporate a priori domain knowledge like monotonicity, shape or unimodality into the fitting process, see \cite{gupta2016monotonic} or \cite{you2017deep}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}

The base of this thesis is a literature study about function fitting using a priori domain knowledge focusing on non-parametric techniques and neural networks. We decided to use structured additive regression \cite{fahrmeir2007regression} utilizing B-splines and tensor-product B-splines as non-linear basis functions. This approach enables flexible, multi-dimensional function fitting. We further expanded this method by applying a priori domain knowledge through the use of user-defined constraints. These constraints consist of mapping matrices determined by the type of constraint, and weighting matrices, determining whether the constraint is active, see \cite{hofner2011monotonicity} and \cite{bollaerts2006simple}.

We are able to incorporate the following a priori domain knowledge: 

\begin{itemize} \label{list:possible-constraints}
	\item Jamming, i.e. $f(x^{(p)}) \approx y^{(p)}$ for some point $p$ with high fidelity
	\item Boundedness, i.e. $f(x) \ge M$ or $f(x) \le M$ for some value M
	\item Monotonicity, i.e. $f'(x) \ge 0$ or $f'(x) \le 0$
	\item Curvature, i.e. $f''(x) \ge 0$ or $f''(x) \le 0$
	\item Unimodality, i.e. \\ $f'(x) > 0, \ x < m$ and $f'(x) < 0, \ x > m$ for $m = \arg \max_{x} f(x)$ or \\ $f'(x) < 0, \ x < m$ and $f'(x) > 0, \ x > m$ for $m = \arg \min_{x} f(x)$
\end{itemize}
%
The thesis is divided into 5 chapters: Chapter 2 provides an overview of the fundamental mathematical concepts used. We focus on the description of linear models, model selection and B-splines as well as on the topic of structured additive regression. In chapter 3, we present the algorithm to incorporate a priori domain knowledge using the concepts given in chapter 2. In chapter 4, we show some aspects of the practical application of the algorithm on noisy and sparse data. Chapters 5 and 6 provide examples using real-word data. In Chapter 7, we give a summary and outline future, possible work. 
