\chapter{Introduction} \label{cha:introduction}

The digitalization of complex, large-scale industrial plants is leading to a massive increase of process data. This data can be used to enhance the overall understanding of the characterizing physical process inside the plant. Modern observer and control concepts are used to improve the efficiency of the plant. They are based on mathematical models of the underlying process characterized by a high accuracy and low computational effort to enable real-time applications. An exact physical description of the relevant quantities as mathematical model is nevertheless often not feasible due to the complexity of the process as well as computational and measurement limitations.

Data-driven approaches are state-of-the-art in many fields, including e.g. image and speech recognition. The usage of data-driven methods, e.g. artificial neural networks, parametric models, etc., to model process quantities of interest for which measurements are too expensive or not practical, also gains more and more influence and acceptance in the field of process control and optimization. The application of any specific data-driven algorithm depends on issues like data quality, interpretability of the model and computational efficiency. 

In most process optimization tasks, specific domain knowledge in form of physical theories and a priori knowledge is available. The combination of the use of physics-based (domain knowledge) and data-driven modeling techniques is called hybrid modeling or grey-box modeling. It lies between the two modeling extrema of white-box models, which are derived from first principles, and black-box models, which are derived from data only \cite{ashby1961introduction}. The incorporation of the domain specific knowledge, also known as a priori knowledge, in state-of-the-art data-driven approaches is not trivial and not solved for most algorithms. Nevertheless, its inclusion should improve the interpretability of the model, which itself is of importance in the context of the emerging field of explainable artificial intelligence (XAI). XAI refers to modeling approaches and techniques in which the main goal is that the resulting model is understandable by humans \cite{dovsilovic2018explainable}.

In this thesis, we present an algorithm for efficient, static, multi-dimensional function approximation using a priori domain knowledge. The algorithm is based on structured additive regression using B-splines \cite{fahrmeir2007regression}. We use user-defined constraints to include a priori domain knowledge in the fitting process, see \cite{hofner2011monotonicity} and \cite{bollaerts2006simple}. We produce interpretable and efficient models based on the given domain knowledge. The incorporation of domain specific knowledge improves the model quality and robustness as well as the interpolation behavior in situations where measured data is sparse and/or noisy. Furthermore, we evaluate the algorithm using noisy samples from artificial test functions with known behavior as well as real-world data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

We will now discuss commonly used data-driven algorithms. The discussion includes the following model approaches:

\begin{itemize}
	\item Parametric models: Linear and polynomial regression
	\item Non-parametric models: Basis function models
	\item Gaussian process regression
	\item Artificial neural networks
	\item Look-up tables
\end{itemize}
%
We will focus on the interpretability, computational efficiency and the ability to include domain knowledge of the individual modeling approaches. The list given above is not intended to give a complete overview of the field of data-driven modelling but rather to be an introduction.

The common starting point for the different data-driven modeling approaches is that we have some multi-variate data $\mathcal{D}$, i.e.

\begin{align}
	\mathcal{D} = \left\{ (x_1^{(i)}, \dots, x_q^{(i)}; y^{(i)} ), \ i = 1, \dots, n\right\},
\end{align} 
%
where $x_1, \dots, x_q$ are the inputs, $y^{(i)}$ is the measured output and $n$ is the number of measurement samples. For ease of presentation, we restrict the following discussion to the single-input setting, i.e. $(x^{(i)}, y^{(i)}), \ i=1, \dots, n$. The generalization to multiple input dimensions is given in the respective literature. We then use the given data to estimate a model function $f(x)$  which is used to predict the response or output variable $y$, i.e.

\begin{align} \label{eq:basic-model-structure}
	y = f(x).
\end{align}
%
Therefore, we are in the setting of supervised learning. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parametric Models}

According to \cite{nelles2013nonlinear}, parametric models are defined as models that can describe the true process behavior using a finite number of parameters. An example is given by the linear regression model for one input variable $x$ as

\begin{align} \label{eq:classical-linear-model}
	y= f(x) = \beta_0 + \beta_1 x.
\end{align}
%
Both parameters $\beta_0$ and $\beta_1$ allow for a direct interpretation as $\beta_0$ is the intercept, i.e. the output for the input $x=0$, and $\beta_1$ is the slope, i.e. the constant defining the relationship between the increase of the output $y$ with respect to the increase of the input $x$.  The interpretability of linear regression models is therefore very high. 

Linear regression models are widely used and part of standard software tools. Their parameters can be efficiently computed using the least squares algorithm. One major drawback is that they can only recover linear relationships between input and output variables. They are therefore quite restrictive and do not allow the incorporation of a priori domain knowledge except being increasing or decreasing by the sign of the slope $\beta_1$. 

An extension of the linear regression model is given by polynomial regression. Here, we try to model the output data $y$ using a polynomial of degree $p$, i.e.

\begin{align} \label{eq:polynomial-model}
	y = f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_p x^p.
\end{align}
%
Polynomial regression introduces more flexibility in the fitting process, since the restriction of linear relationship is relaxed to a polynomial relationship of degree $p$. As for linear models, the interpretability of the parameters is given. We can use the least squares algorithm for parameter estimation. The incorporation of some a priori domain knowledge is possible, e.g. as the degree of the polynomial regression model. The major problem of polynomial regression is that the model function becomes quite wiggly for high polynomial degrees $p$. 

Linear and polynomial regression models are so-called global models. Their parameters act on the complete input space. This property makes the incorporation of specific a priori domain knowledge, e.g. unimodal behavior, difficult and in most cases nearly impossible for parametric models. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Non-parametric Models}

In \cite{nelles2013nonlinear}, non-parametric models are defined as models which require an infinite number of parameters to describe a process exactly. In almost all practical applications this infinite series is approximated by a finite number of parameters using the basis function approach given by

\begin{align} \label{eq:basis-function-approach}
	y = f(x) = \sum_{i=1}^d \Phi_i(x, \theta_i) \beta_i,
\end{align} 
%
with the basis functions $\Phi_i(\cdot)$, the parameters $\beta_i$, the input variable $x$ and the basis function parameters $\theta_i$. The output $y$ is therefore given by a linear combination of $d$ basis functions $\Phi_i(\cdot)$. To model a nonlinear relationship between $y$ and $x$, the basis functions $\Phi_i(\cdot)$ need to be nonlinear. Commonly used basis functions are, e.g. the \emph{hat function}, the \emph{Gaussian}, \emph{splines} or the \emph{hinge function} \cite{friedman2001elements}. 

A widely used algorithm utilizing the basis function approach is called Multivariate Adaptive Regression Splines (MARS) \cite{friedman1991multivariate}. MARS approximates data, as example again for a single input dimension, using the following model

\begin{align} \label{eq:MARS}
	y = \sum_{i=1}^d \Phi_i(x) \theta_i
\end{align}
%
with the constant parameters $\theta_i$. The basis functions $\Phi_i(\cdot)$ are one of the following three alternatives:

\begin{enumerate}
	\item $\Phi_i(x) = 1$, representing the intercept.
	\item $\Phi_i(x) = \max(0, x - c_i)$ or $= \max(0, c_i - x)$, representing the \emph{hinge function} $h_i$ using the constant value $c_i$.
	\item $\Phi_i(x) = h_i  h_j$, representing a product of two \emph{hinge functions}.
\end{enumerate}
%
MARS fits the model by a recursive splitting approach. More information can be found in \cite{friedman2001elements} and \cite{friedman1991multivariate}. MARS models are more flexible compared to the parametric linear and polynomial regression models. As only hinge functions and products of hinge functions are used, MARS models are efficient and in general simple to understand and interpret. To our knowledge, there is currently no possibility to include a priori domain knowledge in the fitting process when using MARS. 

Another popular method utilizing basis functions is the use of \emph{splines}. Splines are defined as piece-wise polynomials on a sequence of knots. Further information can be found in Section~\ref{sec:Splines} and \cite{deBoor1978practicalGuideToSplines}. We focus on the so-called B-splines or basis-splines. Using a B-spline consisting of $d$ B-spline basis functions of order $l$ to model some data, we obtain the model formulation as

\begin{align} \label{eq:Spline-basis-formulation}
	y = f(x) = \sum_{j=1}^d B_j^l(x) \beta_j, 
\end{align}
%
with the B-spline basis functions $B_j^l$ and the parameters $\beta_j$. The parameters can be easily calculated by the least squares algorithm. The usage of splines allows a lot of flexibility and computational efficiency. A priori domain knowledge can be incorporated using an iterative variante of the penalized least squares algorithm with a sophisticated choice of mapping and weighting matrices, see~\pref{cha:solution-approach}, and, e.g. \cite{hofner2011monotonicity} or \cite{bollaerts2006simple}.

The basis function approach in (\ref{eq:basis-function-approach}) may be extended by changing the parameters $\beta_i$ to more complex forms. An example for this is the so-called local linear neuro-fuzzy model, for which each parameter $\beta_i$ is changed to be a \emph{local linear model} and each basis function $\Phi_i(\cdot)$ is then called \emph{validity function} determining the region of validity of the local linear model \cite{nelles2013nonlinear}. The validity functions are normalized for any model input $x$, i.e.

\begin{align} \label{eq:LILOMOT-normalized-basis-fucntions}
	\sum_{i=1}^d \Phi_i(x) = 1
\end{align}
%
and typically chosen to be \emph{Gaussian} functions, i.e. 

\begin{align} \label{eq:validity-function}
	\Phi_i(x) = a_i \exp \left(\frac{(x - \mu_i)^2}{\sigma_i^2} \right),	
\end{align}
%
with the normalization constant $a_i$ and the parameters $\mu_i$ and $\sigma_i$ determining the location and scale of the Gaussian function. The output of the local linear neuro-fuzzy model using $d$ local linear models is then given by

\begin{align} \label{eq:LOLIMOT}
	y = \sum_{i=1}^d  \Phi_i(x) \left(\beta_{i0} + \beta_{i1} x_1\right).
\end{align}
%
The second term in the summation are the \emph{local linear models}. The parameters $\beta_{ij}$ for $i=1, \dots, d$ and $j=0, 1$ as well as the parameters $\mu_i$ and $\sigma_i$ from the validity functions $\Phi_i$ need to be optimized. This is done in the LOLIMOT algorithm, see \cite{nelles2013nonlinear}. 

Local linear models as extension of linear models possess more flexibility with regards to nonlinear relationships in the data. They can also be efficiently evaluated after the iterative training process. The interpretability is high since each local linear model contributes to the prediction according to its validity function. The ability to include a priori domain knowledge in the fitting process is currently not available.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussian Process Regression}
Gaussian process regression is a non-parametric method using a mean function $m(x)$ and a covariance function $k(x, x')$. The covariance function describes the distance between the two inputs $x$ and $x'$. The commonly used covariance function is the so-called \emph{Squared-Exponential} function, i.e.
 
\begin{align}
 	k_{\mathrm{SE}}(x_m, x_n) = \sigma_f^2 \exp \left(-\frac{\lVert x_m - x_n \rVert^2 }{2l^2}\right) + \sigma_m^2 \delta_{m,n},
\end{align}
%
where $\sigma_f^2$ is the variance of the true function, $\sigma_m^2$ is the variance of the measurement noise and $\delta_{m,n}$ is the Kronecker-Delta. The parameter $l$ tunes the smoothness of the \emph{Squared-Exponential}. 

Since we are in the non-parametric setting, we use the complete training data $(\vec{X}, \vec{y})$, with the input $\vec{X} = [x_1^{(i)}, \dots, x_q^{(i)}]$ and the output $\vec{y} = [y^{(i)}]$ for $i=1, \dots, n$,  to evaluate the Gaussian Process at a new data point $\vec{x} = [x_1, \dots, x_q]$, see \cite{bergmann2019gaussprozessregression} and \cite{rasmussen2005GPforML}. To do this, we use the joint distribution between the training data and the new data point, i.e.

\begin{align}
	\begin{bmatrix}
		\vec{y} \\ y 
	\end{bmatrix} \sim \mathcal{N} \left( m(\vec{x}), \begin{bmatrix} k(\vec{X}, \vec{X}) & k(\vec{X}, \vec{x}) \\
																		 k(\vec{x}, \vec{X}) & k(\vec{x}, \vec{x}) 
														\end{bmatrix} \right),
\end{align} 
%
to calculate the mean value and the variance of the data point $\vec{x}$ as

\begin{align} \label{eq:GP-mean}
	\text{E}(\vec{x}) = m(\vec{x}) + k(\vec{x}, \vec{X}) (k(\vec{X}, \vec{X}))^{-1} (\vec{y} - m(\vec{X}))
\end{align}
%
and

\begin{align} \label{eq:GP-var}
	\text{Var}(\vec{x})  = k(\vec{x}, \vec{x}) - k(\vec{x}, \vec{X}) (k(\vec{X}, \vec{X}))^{-1}k(\vec{X}, \vec{x}).
\end{align}
%
The main issue with Gaussian Process regression is that it allows for the incorporation of a priori knowledge through the definition of the covariance function $k(x,x')$. An example can be found in the standard textbook on Gaussian Process regression in \cite{rasmussen2005GPforML}, in which they fit the Mauna Loa Atmospheric Carbon Dioxide data set using a complex covariance function derived by combining simple, interpretable covariance functions.  Nevertheless, as seen~\pref{eq:GP-mean} and~\pref{eq:GP-var}, we need the information of the whole training data set to evaluate the mean value and the covariance which may lead to numerical issues for large data sets. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Artificial Neural Networks}

Artificial neural networks are currently the state-of-the-art solution method for many problems ranging from computer vision over time-series prediction to regression tasks. They are constructed as coarse model of the human brain, consisting of neurons which are connected by some weights. These connections are adapted in the learning process using an algorithm called "backpropagation". They utilize a high number of parameters to model high-dimensional relationships in the data. Further information can be found in standard textbook about neural networks, e.g. \cite{bishop2006patternRecognition} or \cite{goodfellow2016deep}. 

In terms of modeling flexibility, artificial neural networks of sufficient size are proven to be able to represent a wide variety of functions by so-called universal approximation theorems, cf. \cite{cybenko1989approximation} and \cite{hornik1991approximation}. The computational complexity of a neural network depends on its size, aka. the number of parameters. Large networks need many training samples to generate sufficiently accurate predictions. Artificial neural networks are an example of a black-box model. The inclusion of a priori domain knowledge into the learning process of neural networks is possible for specific types of knowledge using the concepts of hints, see \cite{abu1990learning} and \cite{sill1997monotonicity}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Look-up Tables}

A look-up table is an array of values, which allows to replace computational expensive computations with inexpensive array indexing operations. The values in the look-up table are most often computed and stored beforehand. To gain higher resolution, interpolation techniques such as linear or quadratic interpolation may be applied to look-up tables. 

Look-up tables are a standard tool in many fields. They are extremely efficient in terms of computation time. One problem that occurs is the exponential increase in size with the number of dimensions for the look-up table. As example, a $2 \times 2$-table needs to save 4 values, while a $2 \times 2 \times 2$ table already needs 8 values without gaining additional accuracy. Another issue is that the values in the look-up table may come from computationally expensive or complex physical models. The creation of the look-up table is then time-consuming.  

Lattice regression tackles this problems by jointly estimating all look-up table values by minimizing the regularized interpolation error on training data \cite{garcia2009lattice}. They state that using ensembles of look-up tables which combine several \emph{tiny} lattices enables linear scaling in the number of input dimension even for high dimensions \cite{fard2016fast}. They further state that lattice regression may be used to incorporate a priori domain knowledge like monotonicity, shape or unimodality into the fitting process, see \cite{gupta2016monotonic} or \cite{you2017deep}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}

The base of this thesis is a literature study about function fitting using a priori domain knowledge focusing on non-parametric techniques and neural networks. We decided to use structured additive regression \cite{fahrmeir2007regression} utilizing B-splines and tensor-product B-splines as non-linear basis functions since these offer a computational efficient implementation due to their locality feature. This approach enables flexible, multi-dimensional function fitting. We further expanded this method by applying a priori domain knowledge through the use of user-defined constraints. These constraints consist of mapping matrices determined by the type of constraint, and weighting matrices, determining whether the constraint is active, see \cite{hofner2011monotonicity} and \cite{bollaerts2006simple}.

We are able to incorporate the following a priori domain knowledge: 

\begin{itemize} \label{list:possible-constraints}
	\item Jamming, i.e. $f(x^{(p)}) \approx y^{(p)}$ for some point $p$ with high fidelity
	\item Boundedness, i.e. $f(x) \ge \mathrm{B}$ or $f(x) \le \mathrm{B}$ for some bound B
	\item Monotonicity, i.e. $f'(x) \ge 0$ or $f'(x) \le 0$
	\item Curvature, i.e. $f''(x) \ge 0$ or $f''(x) \le 0$
	\item Unimodality, i.e. \\ $f'(x) > 0, \ x < m$ and $f'(x) < 0, \ x > m$ for $m = \arg \max_{x} f(x)$ or \\ $f'(x) < 0, \ x < m$ and $f'(x) > 0, \ x > m$ for $m = \arg \min_{x} f(x)$
\end{itemize}
%
The thesis is divided into 6 chapters: Chapter 2 provides an overview of the fundamental mathematical concepts used. We focus on the description of linear models, model selection and B-splines as well as on the topic of structured additive regression. In chapter 3, we present the algorithm to incorporate a priori domain knowledge using the concepts given in chapter 2. In chapter 4, we show some aspects of the practical application of the algorithm on noisy and sparse data. Chapter 5 provides examples using real-world data. In Chapter 6, we give a summary and outline to future, possible work. 
