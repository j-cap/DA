\chapter{Solution Approach} \label{cha:solution-approach}

The main goal of this thesis is the development and implementation using Matlab and Python of an algorithm to include a priori domain knowledge like monotonicity, curvature, unimodality, etc. into the data fitting process. Using  this additional information should improve the generalization capabilities of the model in situations of sparse, noisy or even partly wrong data. In this chapter, we use the theory discussed in~\pref{cha:fundamentals}, i.e. B-splines in Section~\ref{subsec:b-splines} and P-splines in Section~\ref{subsec:p-splines}, and extend it by adding a shape-constraint penalty term of the form 

\begin{align} \label{eq:constraint-penalty-term}
	\lambda_c \cdot \text{con}(\vec{\beta})
\end{align} 
%
depending on the user-defined a priori domain knowledge leading to the so-called \emph{shape-constraint P-splines} (SCP-splines). The various types of a priori domain knowledge that can be included are listed in Table~\ref{tab:constraint_overview}.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|ll|l|}
		\hline
		\textbf{Constraint}& & \textbf{Description}   & \textbf{Section}     \\ \hline \toprule
		Jamming            & & $f(x^{(p)}) \approx y^{(p)}$ & \pref{subsec:JammC} \\ \hline 
		\multirow{2}{*}{Boundedness}  & lower & $f(x)\ge B$ 	  &	\pref{subsec:BoudC} \\ \cline{2-4}
		& upper & $f(x)\le B$    & \pref{subsec:BoudC} \\ \hline
		\multirow{2}{*}{Monotonicity} & increasing & $f'(x) \ge 0$ 	& \pref{subsec:MIC} \\ \cline{2-4}
		& decreasing & $f'(x) \le 0$  & \pref{subsec:MDC} \\ \hline	
		\multirow{2}{*}{Curvature}    & convex     & $f''(x)\ge 0$ 	& \pref{subsec:ConvC} \\ \cline{2-4}
		& concave    & $f''(x)\le 0$ 	& \pref{subsec:ConcC} \\ \hline
		\multirow{6}{*}{Unimodality}  & \multirow[t]{3}{*}{peak}  & $m = \arg \max_{x} f(x)$  & \pref{subsec:PeakC} \\ 
		&	                       & $f'(x) \ge 0 \quad \text{if} \ x < m$ & \\ 
		&  				       & $f'(x) \le 0 \quad \text{if} \ x > m$ & \\ \cline{2-4} 
		& \multirow[t]{3}{*}{valley}& $m = \arg \min_{x} f(x)$  & \pref{subsec:ValleyC} \\ 
		&	                       & $f'(x) \le 0 \quad \text{if} \ x < m$ & \\ 
		&  				       & $f'(x) \ge 0 \quad \text{if} \ x > m$ &  \\ \hline		\bottomrule
	\end{tabular}
	\caption{Overview of the considered constraints.}
	\label{tab:constraint_overview}
\end{table}
%
The focus of this chapter is the definition of shape-constraint P-splines, see Section~\ref{sec:SCP-splines}, which are characterized by their parameters $\vec{\beta}$ given by solving the optimization problem

\begin{align} \label{eq:OF-SCP-splines}
	\text{PLS}_{\text{SC}} (\vec{y}, \vec{\beta}_b; \lambda, \lambda_c) = \lVert \vec{y} - \vec{B} \vec{\beta}_b \rVert_2^2 + \lambda \cdot \text{pen}(\vec{\beta}_b) + \lambda_c \cdot \text{con}(\vec{\beta}_b),
\end{align}
%
where $\text{pen}(\vec{\beta}_b)$ is the smoothness penalty term for P-splines, see Section~\ref{subsec:p-splines}, and $\text{con}(\vec{\beta}_b)$ specifies the user-defined shape-constraint penalty term to incorporate a priori domain knowledge, see \cite{hofner2011monotonicity} and \cite{bollaerts2006simple}. We further extend the concept proposed in literature of shape-constraint P-splines to two dimensions and discuss shape-constraint tensor-product P-splines, see Section~\ref{sec:SCP-tp-splines}.

\section{Shape-constraint P-splines} \label{sec:SCP-splines}

In Section~\ref{subsec:p-splines}, we enforce smoothness by penalizing the second-order derivative of the underlying B-spline using finite differences of adjacent parameters over the whole input space to create the so-called P-splines. We will now utilize the same idea to create the shape-constrained penalty term $\text{con}(\vec{\beta}_b)$ in~\pref{eq:OF-SCP-splines}. We motivate the approach using the example of monotonic increasing behavior. The descriptions for the other constraints listed in~\pref{tab:constraint_overview} follow afterwards. In the following discussion, we use $d$ B-spline basis functions of order $l$ and the data set $\mathcal{D} = \{(x^{(i)}, y^{(i)}), \ i=1,2,\dots,n\}$ resulting in the B-spline basis matrix $\vec{X} \in \mathbb{R}^{n \times d}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monotonic increasing constraint} \label{subsec:MIC}

A function is monotonic increasing if its first-order derivative is larger than or equal to zero for the whole input space. We therefore introduce a penalty of the form

\begin{align} \label{eq:SCP-penalty-base-form}
	\lambda_c \int \left(f(x)' \right)^2 \,\mathrm{d}x \quad \text{if} \ f'(x) < 0,
\end{align} 
%
to penalize negative first-order derivatives of the estimated function. The penalty is weighted by the \emph{constraint parameter} $\lambda_c$. Once again, we make use of the finite difference approximation, see Section~\ref{subsec:p-splines}. Now, the first-order derivative leads to a penalty of the form

\begin{align} \label{eq:SCP-penalty-matrix-form}
	\lambda_c \cdot \text{con}(\vec{\beta}_b) = \lambda_c \transpose{\vec{\beta}}_b \vec{K}_c \vec{\beta}_b,
\end{align}
% 
with the shape-constraint penalty matrix $\vec{K}_c = \transpose{\vec{D}}_1 \vec{V}_c \vec{D}_1 \in \mathbb{R}^{d \times d}$. The first-order derivative is approximated using the matrix form of the first-order finite difference operator $\Delta^1 \beta_j = \beta_j - \beta_{j-1}$ given by

\begin{align}\label{eq:FD-operator-order-1}
	\vec{D}_1 \vec{\beta}_b = 
	\begin{bmatrix}
		-1 &  1 &   \\
           & \ddots & \ddots \\
		   &        &  -1  & 1   
	\end{bmatrix} 
	\begin{bmatrix}
		\beta_1 \\
		\vdots \\
		\beta_d
	\end{bmatrix},
\end{align}
%
with the difference matrix $\vec{D}_1 \in \mathbb{R}^{(d-1) \times d}$. A weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-1)\times(d-1)}$ is introduced to handle the if-condition in~\pref{eq:SCP-penalty-base-form}. It is a diagonal matrix with the diagonal elements $v_j$ defined as

\begin{align} \label{eq:weighting-matrix-inc-diagonal}
	v_{j-1}(\vec{\beta}_b) = \begin{cases}
		0, \quad \text{if} \ \Delta^1\beta_j \ge 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j < 0
	\end{cases}	\ \text{for} \ j=2,3, \dots, d.
\end{align}
% 
Therefore, the weighting matrix $\vec{V}_c \coloneqq \vec{V}_c(\vec{\beta}_b)$ depends on the parameters $\vec{\beta}_b$ and we arrive at a formulation similar to Ridge regression with a parameter-dependent, non-linear penalty matrix $\vec{K} \coloneqq \vec{K}(\vec{\beta}_b)$, see Section~\ref{subsec:Regularization}. The objective function is finally of the form

\begin{align} \label{eq:OF-SCP-Final}
	\text{PLS}_{\mathrm{SCP}} (\vec{y}, \vec{\beta}_b; \lambda, \lambda_c) = \lVert \vec{y} - \vec{B} \vec{\beta}_b \rVert_2^2 + \lambda \transpose{\vec{\beta}}_b \vec{K} \vec{\beta}_b + \lambda_c \transpose{\vec{\beta}}_b \vec{K}_c \vec{\beta}_b.
\end{align}
%
We use the iterative approach given in Algorithm~\pref{alg:scp} to estimate the optimal parameters $\hat{\vec{\beta}}_{SC}$ under the user-defined shape constraint of monotonic increasing behavior.

\begin{algorithm}[H]
	\SetAlgoLined
	\KwResult{$\hat{\vec{\beta}}_{\mathrm{SC}}$}
	$ i \gets 1$\;
	$\hat{\vec{\beta}}_{i} \gets$ Solution from~\pref{eq:OF-SCP-Final} for $\lambda = \lambda_c = 0$\;
	$\vec{V}^0_c \gets \vec{0}$\;
	$\vec{V}^1_c \gets \vec{V}_c(\hat{\vec{\beta}}_{i})$\;

	\While{$\vec{V}_c^i \ne \vec{V}_c^{i-1}$}{
		$\hat{\vec{\beta}}_{i+1} = (\transpose{\vec{X}} \vec{X} + \lambda \transpose{\vec{D}}_2 \vec{D}_2 + \lambda_c \transpose{\vec{D}}_c \vec{V}^i_c \vec{D}_c)^{-1} \transpose{\vec{X}} \vec{y}$\;
		$\vec{V}^{i+1}_c \gets \vec{V}_c(\hat{\vec{\beta}}_{i+1})$\;	
		$i \gets i+1$\;
	}
	$\hat{\vec{\beta}}_{\mathrm{SC}} \gets \hat{\vec{\beta}}_{i}$\;
	\caption{Estimation of the shape-constraint P-spline coefficients.}
	\label{alg:scp}
\end{algorithm}
%
In Algorithm~\pref{alg:scp}, we make use of a Newton-Raphson scheme for the estimation of $\hat{\vec{\beta}}_{i+1}$. For more information, see~\pref{apx:AppendixB} and \cite{bollaerts2006simple}. The approach described above incorporates the shape-constraint as soft constraint depending on the constraint parameter $\lambda_c$ with the limits of no constraint for $\lambda_c \rightarrow 0$ and hard constraint for $\lambda_c \rightarrow \infty$. Therefore, $\lambda_c$ should be set by hand reflecting the user's confidence in  the a priori domain knowledge. To incorporate the other constraints listed in~\pref{tab:constraint_overview}, we need to specify the shape-constraint  penalty matrix $\vec{K}_c$ depending on the respective constraint, i.e. we define the constraint specific mapping matrix $\vec{D}_c$ and weighting matrix $\vec{V}_c$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monotonic decreasing constraint} \label{subsec:MDC}

Monotonic decreasing behavior can be introduced by penalizing positive first-order derivatives. Therefore, we use the matrix form of the first-order finite difference operator given in~\pref{eq:FD-operator-order-1} as mapping matrix $\vec{D}_1 \in \mathbb{R}^{(d-1) \times d}$ and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-1) \times (d-1)}$ as

\begin{align} \label{eq:weighting-matrix-dec-diagonal}
	v_{j-1}(\vec{\beta}_b) = \begin{cases}
		0, \quad \text{if} \ \Delta^1\beta_j \le 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j > 0
	\end{cases},	\ \text{for} \ j=2,3, \dots, d.
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convex constraint} \label{subsec:ConvC}

Convex behavior can be introduced by penalizing negative second-order derivatives. Therefore, we  use the matrix form of the second-order finite difference operator $\Delta^2 \beta_j = \beta_j - 2\beta_{j-1} + \beta_{j-2}$ given by

\begin{align} \label{eq:FD-operator-order-2}
	\vec{D}_2 \vec{\beta}_b = 
	\begin{bmatrix} 
		1& -2     & 1   \\  
		 & \ddots & \ddots  & \ddots \\ 
		 &        & 1       & -2     & 1 
	\end{bmatrix} \begin{bmatrix}
		\beta_1 \\
		\vdots \\
		\beta_d
	\end{bmatrix},
\end{align}
%
with the mapping matrix $\vec{D}_2 \in \mathbb{R}^{(d-2)\times d}$ and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-2) \times (d-2)}$ as

\begin{align} \label{eq:weighting-matrix-conv-diagonal}
	v_{j-2}(\vec{\beta}_b) = \begin{cases}
		0, \quad \text{if} \ \Delta^2\beta_j \ge 0 \\ 
		1, \quad \text{if} \ \Delta^2\beta_j < 0
	\end{cases}, \ \text{for} \ j=3,4, \dots, d.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Concave constraint} \label{subsec:ConcC}

Concave behavior can be introduced by penalizing positive second-order derivatives. Therefore, we  use the matrix form of the second-order finite difference operator, see~\pref{eq:FD-operator-order-2}, as mapping matrix $\vec{D}_2 \in \mathbb{R}^{(d-2)\times d}$ and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-2) \times (d-2)}$ as

\begin{align} \label{eq:weighting-matrix-conc-diagonal}
	v_{j-2}(\vec{\beta}_b) = \begin{cases}
		0, \quad \text{if} \ \Delta^2\beta_j \le 0 \\ 
		1, \quad \text{if} \ \Delta^2\beta_j > 0
	\end{cases}, \ \text{for} \ j=3,4, \dots, d.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Peak constraint} \label{subsec:PeakC}

Peak behavior can be introduced by penalizing negative first-order derivatives for the increasing part and positive first-order derivatives for the decreasing part of the function. Therefore, we use the matrix form of the first-order finite difference operator, see~\pref{eq:FD-operator-order-1}, as mapping matrix $\vec{D}_1 \in \mathbb{R}^{(d-1)\times d}$. The weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-1) \times (d-1)}$ now has a special structure. First, we find the data point $x^{(\text{max})}$ corresponding to the peak value in the data by finding its data index $\mathrm{max}$, i.e. $\text{max} = \max_i y^{(i)} \ \text{for} \ i=1,2,\dots, n$. Next, we automatically identify the dominant B-spline basis function $B_p^l(x)$ around $x^{(\text{max})}$, i.e. the B-spline basis function with the maximal value at $x^{(\text{max})}$, such that

\begin{align}
	B_p^l(x^{(\text{max})}) \ge B_j^l(x^{(\text{max})}), \quad \text{for} \ j=1,2,\dots,d.
\end{align} 
%
We now use the index $p$ of the dominant B-spline basis function in the definition of the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-1) \times (d-1)}$ as 

\begin{align}\label{eq:v_peak_1}
	v_{j-1}(\vec{\beta}_b) &= \begin{cases} 
		0, \quad \text{if} \ \Delta^1\beta_j \ge 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j  < 0
	\end{cases}, \quad \text{for} \ j=2,3,\dots,p
\end{align}
%
and 

\begin{align}\label{eq:v_peak_2}
	v_{j-1}(\vec{\beta}_b) &= \begin{cases} 
		0, \quad \text{if} \ \Delta^1\beta_j \le 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j > 0
	\end{cases}, \quad \text{for} \ j=p+1,p+2,\dots,d.
\end{align}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Valley constraint} \label{subsec:ValleyC}

Valley behavior can be introduced by the same approach as above by multiplying the data with $-1$ or by always doing the inverse operation. Therefore, we use the matrix form of the first-order finite difference operator, see~\pref{eq:FD-operator-order-1}, as mapping matrix $\vec{D}_1 \in \mathbb{R}^{(d-1) \times d}$ and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d-1) \times (d-1)}$ as

\begin{align}\label{eq:v_valley_1}
	v_{j-1}(\vec{\beta}_b) &= \begin{cases} 
		0, \quad \text{if} \ \Delta^1\beta_j \le 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j > 0
	\end{cases}, \quad \text{for} \ j=2,3,\dots,p
\end{align}
%
and 

\begin{align}\label{eq:v_valley_2}
	v_{j-1}(\vec{\beta}_b) &= \begin{cases} 
		0, \quad \text{if} \ \Delta^1\beta_j \ge 0 \\ 
		1, \quad \text{if} \ \Delta^1\beta_j < 0
	\end{cases}, \quad \text{for} \  j=p+1,p+2,\dots,d,
\end{align}
%
for $p$ being the index of the dominant B-spline basis function $B_p^l(x)$ around $x^{(\text{min})}$ with $\text{min} = \min_i y^{(i)} \ \text{for} \ i=1,2,\dots,n$ as data index of the valley value. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Jamming constraint} \label{subsec:JammC}

Jamming the function $f(x)$ by some point $p = (x^{(p)}, y^{(p)})$ means that the estimated function $f(x^{(p)}) \approx y^{(p)}$. This can be incorporated using the B-spline basis matrix $\vec{B} \in \mathbb{R}^{n \times d}$ as mapping matrix $\vec{D}_c \in \mathbb{R}^{n \times d}$ and a weighting matrix $\vec{V}_c \in \mathbb{R}^{n \times n}$ with diagonal elements $v_j$ given by

\begin{align} \label{eq:v_jamming}
	v_j(\vec{\beta}_b) = 
	\begin{cases}
		0, \quad \text{if} \ x^{(j)} \ne x^{(p)} \\
		1, \quad \text{if} \ x^{(j)} = x^{(p)} 
	\end{cases}, \text{for} \ j = 1,2,\dots,n.
\end{align} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Boundedness constraint} \label{subsec:BoudC}

The user-defined constraint for boundedness from below by, e.g. $B=0$ uses as mapping matrix $\vec{D}_c \in \mathbb{R}^{n \times d}$ the B-spline basis matrix $\vec{B} \in \mathbb{R}^{n \times k}$. For the weighting matrix $\vec{V}_c \in \mathbb{R}^{n\times n}$, the diagonal weights $v_j$ are defined as

\begin{align} \label{eq:v_boundedness}
	v_j(\vec{\beta}_b) = \begin{cases} 
		0, \quad \text{if} \ f(x^{(j)}) \ge B\\ 
		1, \quad \text{if} \ f(x^{(j)})  < B		
	\end{cases}, \text{for} \ j=1,2,\dots,n.
\end{align}
%
Using different values of $B$ allows us to bound from below from any number $B$. Switching the comparison operators in~\pref{eq:v_boundedness} enables us to bound functions from above. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shape-constraint tensor-product P-splines}\label{sec:SCP-tp-splines}

In  Section~\ref{subsubsec:tp-splines}, we extended the univariate approach of B-splines to bivariate tensor-product B-splines by using the row-wise Kronecker product, see~\pref{apx:AppendixKroneckerRowWise}. We will now extend the tensor-product P-splines, see Section~\ref{subsec:p-splines}, to incorporate a priori domain knowledge as in~\pref{sec:SCP-splines} into the fitting process. We start by showing how to include a priori domain knowledge in one dimension, see Section~\pref{subsec:MIC-TP-one-dim}, and describe the various constraints listed in~\pref{tab:constraint_overview_2d}. Afterwards, we show how to include a constraint based on both dimensions, see Section~\pref{subsec:MULTICON-TP-one-dim}. In the following discussion, we use $d_1$ and $d_2$ B-spline basis functions of order $l$ for the dimensions 1 and 2 respectively and the data set $D = \{(x_1^{(i)}, x_2^{(i)}, y^{(i)}), \ i=1,2,\dots,n\}$ resulting in the tensor-product B-spline basis matrix $\vec{T} \in \mathbb{R}^{d_1d_2 \times n}$.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|ll|l|}
		\hline
		\textbf{Constraint}& & \textbf{Description}   & \textbf{Section}     \\ \hline \toprule
		\multirow{2}{*}{Monotonicity} & increasing & $ \frac{\partial f(x_1,x_2)}{\partial x_1} \ge 0$ 	& \pref{subsec:MIC-TP-one-dim} \\ \cline{2-4}
		& decreasing & $\frac{\partial f(x_1,x_2)}{\partial x_1} \le 0$  & \pref{subsec:MDC-TP-one-dim} \\ \hline	
		\multirow{2}{*}{Curvature}    & convex     & $\frac{\partial^2 f(x_1,x_2)}{\partial x_1^2}\ge 0$ 	& \pref{subsec:CONV-TP-one-dim} \\ \cline{2-4}
		& concave    & $\frac{\partial^2 f(x_1,x_2)}{\partial x_1^2}\le 0$ 	& \pref{subsec:CONC-TP-one-dim} \\ \hline 
		Multiple  & & & \pref{subsec:MULTICON-TP-one-dim} \\ \hline \bottomrule
	\end{tabular}
	\caption{Overview of the constraints for shape-constraint tensor-product P-splines.}
	\label{tab:constraint_overview_2d}
\end{table}

Note that the constraints \emph{Jamming} and \emph{Boundedness} in~\pref{tab:constraint_overview} are dimension-independent and therefore not listed in~\pref{tab:constraint_overview_2d}. Nevertheless, they can also be applied to tensor-product B-splines using the descriptions in Section~\ref{subsec:JammC} and~\ref{subsec:BoudC}. We will present the scheme for input dimension 1. For dimension 2, minor changes need to be done, e.g. some reordering in the weighting matrix $\vec{V}_c$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Monotonic increasing constraint in dimension 1} \label{subsec:MIC-TP-one-dim}

A function is monotonic increasing in dimension 1 if its first-order derivative in dimension 1 is larger than or equal to zero for the whole input space. Therefore, we introduce a penalty of the form

\begin{align} \label{eq:SCP-tp-penalty-base-from}
	\lambda_c \iint \left( \frac{\partial f(x_1, x_2)}{\partial x_1} \right)^2 \,\mathrm{d}x_1 \,\mathrm{d}x_2 \quad \text{if} \ \frac{\partial f(x_1, x_2)}{\partial x_1} < 0,
\end{align} 
%
to penalize negative first-order partial derivatives of the estimated function. The penalty term is again weighted by the \emph{constraint parameter} $\lambda_c$. Approximating the first-order derivative by finite-differences of order 1, similar to~\pref{eq:row-wise-penalty} and~\pref{eq:col-wise-penalty}, leads to a penalty of the known form 

\begin{align} \label{eq:shape-constraint-inc-dim1}
	\lambda_c \cdot \text{con}(\vec{\beta}_t) = \lambda_c \transpose{\vec{\beta}}_t \vec{K}_c \vec{\beta}_t,
\end{align}
%
with the shape-constraint penalty matrix $\vec{K}_c = \transpose{\vec{D}}_c \vec{V}_c \vec{D}_c \in \mathbb{R}^{d_1d_2 \times d_1d_2}$. The first-order derivative is approximated by the matrix form of the first-order finite difference operator, see~\pref{eq:FD-operator-order-1}, and by using the Kronecker product, cf.~\pref{apx:AppendixKronecker}, as

\begin{align} \label{eq:mapping-matrix-sc-tp-increasing}
	\vec{D}_c \vec{\beta}_t = \big( \vec{I}_{d_2} \otimes \vec{D}_{1}\big) \vec{\beta}_t,
\end{align}
%
with the mapping matrix $\vec{D}_c \in \mathbb{R}^{(d_1-1)d_2 \times d_1d_2}$ using the identity matrix $\vec{I}_{d_2} \in \mathbb{R}^{d_2 \times d_2}$ and the finite-difference matrix for the first dimension $\vec{D}_{1} \in \mathbb{R}^{(d_1-1) \times d_1}$. The diagonal elements $v_j$ of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d_1-1)d_2 \times (d_1-1)d_2}$ are defined as

\begin{align}
	\begin{split}
	v_{j+(i-1)(d_1-1)}(\vec{\beta}_t) ={}& \begin{cases}
		0, \quad \text{if} \ \Delta^1_{d_1} \beta_{j+(i-1)d_1 + 1} \ge 0 \\ 
		1, \quad \text{if} \ \Delta^1_{d_1} \beta_{j+(i-1)d_1 + 1} < 0 
	\end{cases}, \\ {}& \text{for} \ j=1,2,\dots,d_1-1 \ \text{and} \ i=1,2,\dots,d_2,
	\end{split}
\end{align}
%
using the first-order finite-difference operator for dimension 1 $\Delta^1_{d_1}$ defined as

\begin{align} \label{eq:FD-operator-dim1}
	\Delta^1_{d_1} \beta_j = \beta_j - \beta_{j-1}.
\end{align}
%
Hence, we obtain an objective function similar to~\pref{eq:OF-SCP-splines} as

\begin{align} \label{eq:OF-SCP-tp-splines}
	\text{PLS}_{\text{SC-TP}} (\vec{y}, \vec{\beta}_t; \lambda, \lambda_c) = \lVert \vec{y} - \vec{T} \vec{\beta}_t \rVert_2^2 + \lambda \transpose{\vec{\beta}}_t \vec{K} \vec{\beta}_t + \lambda_c \transpose{\vec{\beta}}_t \vec{K}_c \vec{\beta}_t,
\end{align}
%
with the smoothness penalty matrix $\vec{K} \in \mathbb{R}^{d_1d_2 \times d_1d_2}$, see Section~\ref{subsec:p-splines}, and the shape-constraint penalty matrix $\vec{K} \in \mathbb{R}^{d_1d_2 \times d_1d_2}$. The optimal parameters under the shape constraint $\hat{\vec{\beta}}_{\text{SC-TP}}$ can be estimated using the iterative scheme given in Algorithm~\pref{alg:scp}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monotonic decreasing constraint for dimension 1} \label{subsec:MDC-TP-one-dim}

Monotonic decreasing behavior in dimension 1 can be introduced by penalizing positive first-order partial derivatives. Therefore, we use the matrix form of the first-order finite difference operator given in~\pref{eq:FD-operator-order-1}, i.e. $\vec{D}_1 \in \mathbb{R}^{(d_1-1) \times d_1}$, in the set up of the mapping matrix $\vec{D}_c \in \mathbb{R}^{(d_1-1)d_2 \times d_1d_2}$, see~\pref{eq:mapping-matrix-sc-tp-increasing}, and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d_1-1)d_2 \times (d_1-1)d_2}$ as

\begin{align} 
	\begin{split}
	v_{j+(i-1)(d_1-1)} (\vec{\beta}_t) ={}& \begin{cases}
		0, \quad \text{if} \ \Delta^1_{d_1} \beta_{j+(i-1)d_1 + 1} \le 0 \\ 
		1, \quad \text{if} \ \Delta^1_{d_1} \beta_{j+(i-1)d_1 + 1} > 0
	\end{cases},	\\ {}& \text{for} \ j=1,2,\dots,d_1-1 \ \text{and} \ i=1,2,\dots,d_2,
	\end{split}
\end{align}
%
using the first-order finite difference operator for dimension 1 in~\pref{eq:FD-operator-dim1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convex constraint for dimension 1} \label{subsec:CONV-TP-one-dim}

Convex behavior in dimension 1 can be introduced by penalizing negative second-order partial derivatives. Therefore, we  use the matrix form of the second-order finite difference operator in~\pref{eq:FD-operator-order-2}, i.e. $\vec{D}_2 \in \mathbb{R}^{(d_1-2) \times d_1}$, in the set up of the mapping matrix $\vec{D}_c \in \mathbb{R}^{(d_1-2)d_2 \times d_1d_2}$, see~\pref{eq:mapping-matrix-sc-tp-increasing}, and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d_1-2)d_2 \times (d_1-2)d_2}$ as

\begin{align}
	\begin{split}
	v_{j+(i-1)(d_1-2)}(\vec{\beta}_t) {}&= \begin{cases}
		0, \quad \text{if} \ \Delta^2_{d_1} \beta_{j+(i-1)d_1+2} \ge 0 \\ 
		1, \quad \text{if} \ \Delta^2_{d_1} \beta_{j+(i-1)d_1+2} < 0
	\end{cases}	\\ {}& \text{for} \ j=1,2,\dots,d_1-2 \ \text{and} \ i=1,2,\dots,d_2,
	\end{split}
\end{align}
%
using the second-order finite difference operator for dimension 1 defined as

\begin{align} \label{eq:FD-operator2-dim1}
	\Delta^2_{d_1} \beta_j = \beta_j - 2\beta_{j-1} + \beta_{j-2}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Concave constraint for dimension 1} \label{subsec:CONC-TP-one-dim}

Concave behavior in dimension 1 can be introduced by penalizing positive second-order partial derivatives. Therefore, we  use the matrix form of the second-order finite difference operator in~\pref{eq:FD-operator-order-2}, i.e. $\vec{D}_2 \in \mathbb{R}^{(d_1-2) \times d_1}$, in the set up of the mapping matrix $\vec{D}_c \in \mathbb{R}^{(d_1-2)d_2 \times d_1d_2}$, see~\pref{eq:mapping-matrix-sc-tp-increasing}, and define the diagonal elements of the weighting matrix $\vec{V}_c \in \mathbb{R}^{(d_1-2)d_2 \times (d_1-2)d_2}$ as

\begin{align}
	\begin{split}
	v_{j+(i-1)(d_1-2)}(\vec{\beta}_t) {}&= \begin{cases}
		0, \quad \text{if} \ \Delta^2_{d_1} \beta_{j+(i-1)d_1+2} \le 0 \\ 
		1, \quad \text{if} \ \Delta^2_{d_1} \beta_{j+(i-1)d_1+2} > 0
	\end{cases}	\\ {}& \text{for} \ j=1,2,\dots,d_1-2 \ \text{and} \ i=1,2,\dots,d_2,
	\end{split}
\end{align}
%
using the second-order finite difference operator for dimension 1 defined in~\pref{eq:FD-operator2-dim1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Shape constraints in two dimensions} \label{subsec:MULTICON-TP-one-dim}

To enforce shape constraints in two dimensions, e.g monotonic increasing behavior in dimension 1 ($c_1$) and monotonic decreasing behavior in dimension 2 ($c_2$), we use the same idea as given in~\pref{subsec:MIC-TP-one-dim} for both dimensions. Hence, we obtain an objective function similar to~\pref{eq:OF-SCP-splines} as

\begin{align} \label{eq:OF-SCP-tp-splines_2constraints}
	\text{PLS}_{\mathrm{SC-TP}} (\vec{y}, \vec{\beta}_t; \lambda, \lambda_{c_1}, \lambda_{c_2}) = \lVert \vec{y} - \vec{T} \vec{\beta}_t \rVert_2^2 + \lambda \transpose{\vec{\beta}}_t \vec{K} \vec{\beta}_t + \lambda_{c_1} \transpose{\vec{\beta}}_t \vec{K}_{c_1} \vec{\beta}_t + \lambda_{c_2} \transpose{\vec{\beta}}_t \vec{K}_{c_2} \vec{\beta}_t,
\end{align}
%
with the smoothness penalty matrix $\vec{K} \in \mathbb{R}^{d_1d_2 \times d_1d_2}$, see Section~\ref{subsec:p-splines}, and two \emph{constraint parameter} $\lambda_{c_1}$ and $\lambda_{c_2}$ reflecting the users trust in the a priori domain knowledge. The optimal parameters under the shape constraint $\hat{\vec{\beta}}_{\mathrm{SC-TP}}$ can then be estimated again using the iterative scheme given in Algorithm~\pref{alg:scp}.
